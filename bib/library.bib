Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{KENDALL1938,
author = {Kendall, MG},
doi = {10.1093/biomet/30.1-2.81},
file = {::},
issn = {0006-3444},
journal = {Biometrika},
month = {jun},
number = {1},
pages = {81--93},
publisher = {Oxford University Press},
title = {{A new measure of rank correlation}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/30.1-2.81 http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/30.1-2.81{\%}5Cnhttp://www.jstor.org/stable/2332226},
volume = {30},
year = {1938}
}
@article{Killick2014,
abstract = {One of the key challenges in changepoint analysis is the ability to detect multiple changes within a given time series or sequence. The changepoint package has been developed to provide users with a choice of multiple changepoint search methods to use in conjunction with a given changepoint method and in particular provides an implementation of the recently proposed PELT algorithm. This article describes the search methods which are implemented in the package as well as some of the available test statistics whilst highlighting their application with simulated and practical examples. Particular emphasis is placed on the PELT algorithm and how results differ from the binary segmentation approach.},
author = {Killick, Rebecca and Eckley, Idris A.},
doi = {10.18637/jss.v058.i03},
file = {:Users/matt/Documents/Mendeley Desktop/Killick, Eckley - 2014 - changepoint An R Package for Changepoint Analysis.pdf:pdf},
journal = {Journal of Statistical Software},
number = {3},
pages = {1--19},
title = {{changepoint: An R Package for Changepoint Analysis}},
url = {http://www.jstatsoft.org/v58/i03/},
volume = {58},
year = {2014}
}
@article{Killick2011a,
abstract = {We consider the problem of detecting multiple changepoints in large data sets. Our focus is on applications where the number of changepoints will increase as we collect more data: for example in genetics as we analyse larger regions of the genome, or in finance as we observe time-series over longer periods. We consider the common approach of detecting changepoints through minimising a cost function over possible numbers and locations of changepoints. This includes several established procedures for detecting changing points, such as penalised likelihood and minimum description length. We introduce a new method for finding the minimum of such cost functions and hence the optimal number and location of changepoints that has a computational cost which, under mild conditions, is linear in the number of observations. This compares favourably with existing methods for the same problem whose computational cost can be quadratic or even cubic. In simulation studies we show that our new method can be orders of magnitude faster than these alternative exact methods. We also compare with the Binary Segmentation algorithm for identifying changepoints, showing that the exactness of our approach can lead to substantial improvements in the accuracy of the inferred segmentation of the data.},
archivePrefix = {arXiv},
arxivId = {1101.1438},
author = {Killick, R and Fearnhead, P and Eckley, I A},
doi = {10.1080/01621459.2012.737745},
eprint = {1101.1438},
file = {:Users/matt/Documents/Mendeley Desktop/Killick, Fearnhead, Eckley - 2011 - Optimal detection of changepoints with a linear computational cost.pdf:pdf},
isbn = {0162-1459$\backslash$r1537-274X},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Dynamic programming,PELT,Segmentation,Structural change},
pages = {500--1590},
title = {{Optimal detection of changepoints with a linear computational cost}},
url = {http://amstat.tandfonline.com/action/journalInformation?journalCode=uasa20 http://dx.doi.org/10.1080/01621459.2012.737745 http://arxiv.org/abs/1101.1438{\%}0Ahttp://dx.doi.org/10.1080/01621459.2012.737745},
volume = {107},
year = {2011}
}
@book{VanRijsbergen1979,
abstract = {This book is an essential reference to cutting-edge issues and future directions in information retrieval Information retrieval (IR) can be defined as the process of representing, managing, searching, retrieving, and presenting information. Good IR involves understanding information needs and interests, developing an effective search technique, system, presentation, distribution and delivery. The increased use of the Web and wider availability of information in this environment led to the development of Web search engines. This change has brought fresh challenges to a wider variety of users' needs, tasks, and types of information. Today search engines are seen in enterprises, on laptops, in individual websites, in library catalogues, and elsewhere. Information Retrieval: Searching in the 21st Century Information Retrieval Focuses on: •Information Retrieval Models •User-centred Evaluation of Information Retrieval Systems •Multimedia Resource Discovery •Image Users' Needs and Searching Behaviour •Web Information Retrieval •Mobile Search •Context and Information Retrieval •Text Categorisation and Genre in Information Retrieval •Semantic Search •The Role of Natural Language Processing in Information Retrieval: Search for Meaning and Structure •Cross-language Information Retrieval •Performance Issues in Parallel Computing for Information Retrieval This book is an invaluable reference for graduate students on IR courses or courses in related disciplines (e.g. computer science, information science, human-computer interaction, and knowledge management), academic and industrial researchers, and industrial personnel tracking information search technology developmen"This book is an essential reference to cutting-edge issues and future directions in information retrieval Information retrieval (IR) can be defined as the process of representing, managing, searching, retrieving, and presenting information. Good IR involves understanding information needs and interests, developing an effective search technique, system, presentation, distribution and delivery. The increased use of the Web and wider availability of information in this environment led to the development of Web search engines. This change has brought fresh challenges to a wider variety of users' needs, tasks, and types of information. Today, search engines are seen in enterprises, on laptops, in individual websites, in library catalogues, and elsewhere. Information Retrieval: Searching in the 21st Century focuses on core concepts, and current trends in the field. This book focuses on: Information Retrieval Models User-centred Evaluation of Information Retrieval Systems Multimedia Resource Discovery Image Users' Needs and Searching Behaviour Web Information Retrieval Mobile Search Context and Information Retrieval Text Categorisation and Genre in Information Retrieval Semantic Search The Role of Natural Language Processing in Information Retrieval: Search for Meaning and Structure Cross-language Information Retrieval Performance Issues in Parallel Computing for Information Retrieval. This book is an invaluable reference for graduate students on IR courses or courses in related disciplines (e.g. computer science, information science, human-computer interaction, and knowledge management), academic and industrial researchers, and industrial personnel tracking information search technology developments to understand the business implications. Intermediate-advanced level undergraduate students on IR or related courses will also find this text insightful. Chapters are supplemented with exercises to stimulate further thinking"-Provided by publisher. "The aim of this book is to provide a basis for understanding recent developments in the field and to outline directions for information search technologies. The text presents significant contributions to the research community and industry, covering topics such as the semantic search, natural language processing, parallel information retrieval, image and multimedia retrieval, text categorisation, context technology, cross-language retrieval, and mobile information services. The book is a balanced mixture of theory, practice, tools and applications"-Provided by publisher. ts to understand the business implications. Intermediate-advanced level undergraduate students on IR or related courses will also find this text insightful. Chapters are supplemented with exercises to stimulate further thinking.},
author = {van Rijsbergen, C. J.},
booktitle = {Information Retrieval: Searching in the 21st Century},
isbn = {0408709294},
issn = {1386-4564},
pages = {208},
publisher = {Butterworths},
title = {{Information retrieval}},
url = {https://dl.acm.org/citation.cfm?id=539927},
year = {1979}
}
@article{Eckley2011,
abstract = {Many time series are characterised by abrupt changes in structure, such as sudden jumps in level or volatility. We consider change points to be those time points which divide a data set into distinct homogeneous segments. In practice the number of change points will not be known. The ability to detect changepoints are important for both methodological and practical reasons including: the validation of an untested scientific hypothesis (e.g. Henderson and Matthews, 1993); monitoring and assessment of safety critical pro-cesses (e.g. Elsner et al., 2004); and the validation of modelling assumptions (e.g. Fryzlewicz and Subba Rao, 2009). The development of inference methods for change point problems is by no means a recent phenomenon, with early works including Page (1954a), Shiryaev (1963) and Hinkley (1970). Increasingly the ability to detect change points quickly and accu-rately is of interest to a wide range of disciplines. Recent examples of application areas include numerous bioinformatic applications (Lio and Vannucci, 2000; Erdman and Emerson, 2008) the detection of malware within software (Yan et al., 2008), network traffic analysis (Kwon et al., 2006), finance (Spokoiny, 2009), climatology (Jaxk et al., 2007) and oceanography (Killick et al., 2009). In this chapter we describe and compare a number of different approaches for estimating changepoints. For a more general overview of changepoint methods, we refer interested readers to Carlstein et al. (1994) and Chen and Gupta (2000). The structure of this chapter is as follows. First we introduce the model we fo-cus on. We then describe methods for detecting a single changepoint and methods for detecting multiple changepoints, which will cover both frequentist and Bayesian approaches. For multiple changepoint models the computational challenge of per-forming inference is to deal with the large space of possible sets of changepoint positions. We describe algorithms that, for the class of models we consider, can perform inference exactly even for large data sets. In Section 1.4 we look at prac-tical issues of implementing these methods, and compare the different approaches, through a detailed simulation study. Our study is based around the problem of detecting changes in the covariance structure of a time-series, and results suggest that Bayesian methods are more suitable for detection of changepoints, particularly for multiple changepoint applications. The study also demonstrates the advantage of using the exact inference methods. We end with a discussion. Within this chapter we consider the following changepoint models. Let us assume we have time-series data, y 1:n = (y 1 , . . . , y n). For simplicity we assume the obser-vation at each time t, y t , is univariate – though extensions to multivariate data are straightforward. Our model will have a number of changepoints, m, together with their positions, $\tau$ 1:m = ($\tau$ 1 , . . . , $\tau$ m). Each changepoint position is an integer between 1 and n − 1 inclusive. We define $\tau$ 0 = 0 and $\tau$ m+1 = n, and assume that the changepoints are ordered so that $\tau$ i {\textless} $\tau$ j if and only if i {\textless} j. The m changepoints will split the data into m + 1 segments. The ith segment will consist of data y $\tau$i−1+1:$\tau$i . For each segment there will be a set of parameters; the parameters associated with the ith segment will be denoted $\theta$ i . We will write the likelihood function as L(m, $\tau$ 1:m , $\theta$ 1:m+1) = p(y 1:n |m, $\tau$ 1:m , $\theta$ 1:m+1).},
author = {Eckley, I.A. and Fearnhead, P. and Killick, R.},
doi = {10.1017/CBO9780511984679.011},
file = {:Users/matt/Documents/Mendeley Desktop/Eckley, Fearnhead, Killick - 2011 - Analysis of Changepoint Models.pdf:pdf},
isbn = {9780511984679},
journal = {Bayesian Time Series Models},
number = {January},
pages = {205--224},
title = {{Analysis of Changepoint Models}},
year = {2011}
}
@article{Amigo2009,
abstract = {There is a wide set of evaluation metrics available to compare the quality of text clustering algorithms. In this article, we define a few intuitive formal constraints on such metrics which shed light on which aspects of the quality of a clustering are captured by different metric families. These formal constraints are validated in an experiment involving human assessments, and compared with other constraints proposed in the literature. Our analysis of a wide range of metrics shows that only BCubed satisfies all formal constraints. We also extend the analysis to the problem of overlapping clustering, where items can simultaneously belong to more than one cluster. As Bcubed cannot be directly applied to this task, we propose a modified version of Bcubed that avoids the problems found with other metrics.},
author = {Amig{\'{o}}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
doi = {10.1007/s10791-008-9066-8},
file = {:Users/matt/Documents/Mendeley Desktop/Amig{\'{o}} et al. - 2009 - A comparison of extrinsic clustering evaluation metrics based on formal constraints.pdf:pdf},
isbn = {1386-4564 (Print) 1573-7659 (Online)},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Clustering,Evaluation metrics,Formal constraints},
number = {4},
pages = {461--486},
title = {{A comparison of extrinsic clustering evaluation metrics based on formal constraints}},
volume = {12},
year = {2009}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:Users/matt/Documents/Mendeley Desktop/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the Dimension of a Model}},
url = {http://projecteuclid.org/euclid.aos/1176344136},
volume = {6},
year = {1978}
}
@book{Basseville1993,
abstract = {Over the last twenty years, there has been a significant increase in the number of real problems concerned with questions such as fault detection and diagnosis (monitoring); condition-based maintenance of industrial processes; safety of complex systems (aircrafts, boats, rockets, nuclear power plants, chemical technological processes, etc.); quality control; prediction of natural catastrophic events (earthquakes, tsunami, etc.); monitoring in biomedicine.},
author = {Basseville, M and Nikiforov, Igor V},
doi = {10.1016/0967-0661(94)90196-1},
file = {:Users/matt/Documents/Mendeley Desktop/Basseville, Nikiforov - 1993 - Detection of Abrupt Changes Theory and Application.pdf:pdf},
isbn = {0-13-126780-9},
issn = {09670661},
title = {{Detection of Abrupt Changes: Theory and Application}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.6896{\&}rep=rep1{\&}type=pdf},
year = {1993}
}
@article{Tartakovsky2006,
abstract = {Sequential multi-chart detection procedures for detecting changes in multichannel sensor systems are developed. In the case of complete information on pre-change and post-change distributions, the detection algorithm represents a likelihood ratio-based multichannel generalization of Page's cumulative sum (CUSUM) test that is applied to general stochastic models that may include correlated and nonstationary observations. There are many potential application areas where it is necessary to consider multichannel generalizations and general statistical models. In this paper our main motivation for doing so is network security: rapid anomaly detection for an early detection of attacks in computer networks that lead to changes in network traffic. Moreover, this kind of application encourages the development of a nonparametric multichannel detection test that does not use exact pre-change (legitimate) and post-change (attack) traffic models. The proposed nonparametric method can be effectively applied to detect a wide variety of attacks such as denial-of-service attacks, worm-based attacks, port-scanning, and man-in-the-middle attacks. In addition, we propose a multichannel CUSUM procedure that is based on binary quantized data; this procedure turns out to be more efficient than the previous two algorithms in certain scenarios. All proposed detection algorithms are based on the change-point detection theory. They utilize the thresholding of test statistics to achieve a fixed rate of false alarms, while allowing changes in statistical models to be detected "as soon as possible". Theoretical frameworks for the performance analysis of detection procedures, as well as results of Monte Carlo simulations for a Poisson example and results of detecting real flooding attacks, are presented. ?? 2006.},
author = {Tartakovsky, Alexander G. and Rozovskii, Boris L. and Bla{\v{z}}ek, Rudolf B. and Kim, Hongjoong},
doi = {10.1016/j.stamet.2005.05.003},
file = {:Users/matt/Documents/Mendeley Desktop/Tartakovsky et al. - 2006 - Detection of intrusions in information systems by sequential change-point methods.pdf:pdf},
isbn = {1572-3127},
issn = {15723127},
journal = {Statistical Methodology},
keywords = {Change-point detection,Cumulative sum,Denial of service,Intrusion detection,Multichannel information systems,Page's test,Rapid detection,Sequential tests},
number = {3},
pages = {252--293},
title = {{Detection of intrusions in information systems by sequential change-point methods}},
volume = {3},
year = {2006}
}
@article{Kawahara2009,
author = {Kawahara, Yoshinobu and Sugiyama, Masashi},
file = {:Users/matt/Documents/Mendeley Desktop/Kawahara, Sugiyama - 2009 - Change-point detection in time-series data by direct density-ratio estimation.pdf:pdf},
journal = {Proceedings of the 2009 SIAM International Conference on Data Mining},
keywords = {change-point detection,direct density-ratio,estimation,kernel methods,time-series data},
pages = {389--400},
title = {{Change-point detection in time-series data by direct density-ratio estimation}},
year = {2009}
}
@article{Lai1999,
abstract = {This paper addresses a number of open problems concerning the$\backslash$ngeneralized likelihood ratio (GLR) rules for online detection of faults$\backslash$nand parameter changes in control systems. It is shown that with an$\backslash$nappropriate choice of the threshold and window size, these GLR rules are$\backslash$nasymptotically optimal. The rules are also extended to non-likelihood$\backslash$nstatistics that are widely used in monitoring adaptive algorithms for$\backslash$nsystem identification and control by establishing Gaussian$\backslash$napproximations to these statistics when the window size is chosen$\backslash$nsuitably. Recursive algorithms are developed for practical$\backslash$nimplementation of the procedure, and importance sampling techniques are$\backslash$nintroduced for determining the threshold of the rule to satisfy$\backslash$nprescribed bounds on the false alarm rate},
author = {Lai, Tze Leung and Shan, Jerry Zhaolin},
doi = {10.1109/9.763211},
file = {:Users/matt/Documents/Mendeley Desktop/Lai, Shan - 1999 - Efficient recursive algorithms for detection of abrupt changes in signals and control systems.pdf:pdf},
isbn = {0018-9286},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
number = {5},
pages = {952--966},
title = {{Efficient recursive algorithms for detection of abrupt changes in signals and control systems}},
volume = {44},
year = {1999}
}
@inproceedings{Bagga1998,
address = {Morristown, NJ, USA},
author = {Bagga, Amit and Baldwin, Breck},
booktitle = {Proceedings of the 17th international conference on Computational linguistics -},
doi = {10.3115/980451.980859},
file = {::},
pages = {79--85},
publisher = {Association for Computational Linguistics},
title = {{Entity-based cross-document coreferencing using the vector space model}},
url = {http://portal.acm.org/citation.cfm?doid=980451.980859},
volume = {1},
year = {1998}
}
@article{Hubert1985,
abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by review- ing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Milligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from cOrrespond- ing partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addi- tion to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between -4-I},
author = {Hubert, Lawrence and Arabie, Phipps},
doi = {10.1007/BF01908075},
file = {:Users/matt/Documents/Mendeley Desktop/Hubert, Arabie - 1985 - Comparing partitions.pdf:pdf},
isbn = {0176-4268},
issn = {01764268},
journal = {Journal of Classification},
keywords = {Consensus indices,Measures of agreement,Measures of association},
month = {dec},
number = {1},
pages = {193--218},
publisher = {Springer-Verlag},
title = {{Comparing partitions}},
url = {http://link.springer.com/10.1007/BF01908075},
volume = {2},
year = {1985}
}
@manual{FromJedWing2017,
abstract = {Misc functions for training and plotting classification and regression models},
annote = {R package version 6.0-76},
author = {Kuhn, Max and Wing, Jed and Weston, Steve and Williams, Andre and Keefer, Chris and Engelhardt, Allan},
booktitle = {https://Cran.R-Project.Org/Package=Caret},
title = {{Caret: Classification and Regression Training}},
url = {https://cran.r-project.org/web/packages/caret/caret.pdf},
year = {2012}
}
@article{Desobry2005,
abstract = {A number of abrupt change detection methods have been proposed in the past, among which are efficient model-based techniques such as the Generalized Likelihood Ratio (GLR) test. We consider the case where no accurate nor tractable model can be found, using a model-free approach, called Kernel change detection (KCD). KCD compares two sets of descriptors extracted online from the signal at each time instant: The immediate past set and the immediate future set. Based on the soft margin single-class Support Vector Machine (SVM), we build a dissimilarity measure in feature space between those sets, without estimating densities as an intermediary step. This dissimilarity measure is shown to be asymptotically equivalent to the Fisher ratio in the Gaussian case. Implementation issues are addressed; in particular, the dissimilarity measure can be computed online in input space. Simulation results on both synthetic signals and real music signals show the efficiency of KCD.},
author = {Desobry, Fr{\'{e}}d{\'{e}}ric and Davy, Manuel and Doncarli, Christian},
doi = {10.1109/TSP.2005.851098},
file = {:Users/matt/Documents/Mendeley Desktop/Desobry, Davy, Doncarli - 2005 - An online Kernel change detection algorithm.pdf:pdf},
isbn = {0-7803-7663-3},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Abrupt change detection,Kernel method,Music segmentation,Online,Single-class SVM},
number = {8},
pages = {2961--2974},
title = {{An online Kernel change detection algorithm}},
volume = {53},
year = {2005}
}
@misc{trec2016,
author = {{Text Retrieval Conference 2016}},
title = {{TREC 2016 Evaluation Guidelines}},
url = {https://trecrts.github.io/TREC2016-RTS-guidelines.html},
urldate = {2017-07-06}
}
@article{Yao1984,
abstract = {The Cox regression model for censored survival data specifies that covariates have a proportional effect on the hazard function of the life-time distribution of an individual. In this paper we discuss how this model can be extended to a model where covariate processes have a proportional effect on the intensity process of a multivariate counting process. This permits a statistical regression analysis of the intensity of a recurrent event allowing for complicated censoring patterns and time dependent covariates. Furthermore, this formulation gives rise to proofs with very simple structure using martingale techniques for the asymptotic properties of the estimators from such a model. Finally an example of a statistical analysis is included.},
author = {Yao, Yi-Ching},
doi = {10.1214/aos/1176346802},
file = {:Users/matt/Documents/Mendeley Desktop/Yao - 1984 - Estimation of a Noisy Discrete-Time Step Function Bayes and Empirical Bayes Approaches.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Bayes,Change points,empirical Bayes,filtering,smoothing},
month = {dec},
number = {4},
pages = {1434--1447},
publisher = {Institute of Mathematical Statistics},
title = {{Estimation of a Noisy Discrete-Time Step Function: Bayes and Empirical Bayes Approaches}},
url = {http://projecteuclid.org/euclid.aos/1176346802},
volume = {12},
year = {1984}
}
@article{Ginsberg2009,
abstract = {Seasonal influenza epidemics are a major public health concern, causing tens of millions of respiratory illnesses and 250,000 to 500,000 deaths worldwide each year. In addition to seasonal influenza, a new strain of influenza virus against which no previous immunity exists and that demonstrates human-to-human transmission could result in a pandemic with millions of fatalities. Early detection of disease activity, when followed by a rapid response, can reduce the impact of both seasonal and pandemic influenza. One way to improve early detection is to monitor health-seeking behaviour in the form of queries to online search engines, which are submitted by millions of users around the world each day. Here we present a method of analysing large numbers of Google search queries to track influenza-like illness in a population. Because the relative frequency of certain queries is highly correlated with the percentage of physician visits in which a patient presents with influenza-like symptoms, we can accurately estimate the current level of weekly influenza activity in each region of the United States, with a reporting lag of about one day. This approach may make it possible to use search queries to detect influenza epidemics in areas with a large population of web search users.},
author = {Ginsberg, Jeremy and Mohebbi, Matthew H and Patel, Rajan S and Brammer, Lynnette and Smolinski, Mark S and Brilliant, Larry},
doi = {10.1038/nature07634},
file = {:Users/matt/Documents/Mendeley Desktop/Ginsberg et al. - 2009 - Detecting influenza epidemics using search engine query data.pdf:pdf},
isbn = {1476-4687 (Electronic)},
issn = {1476-4687},
journal = {Nature},
keywords = {Centers for Disease Control and Prevention (U.S.),Databases, Factual,Health Behavior,Health Education,Health Education: statistics {\&} numerical data,Humans,Influenza, Human,Influenza, Human: diagnosis,Influenza, Human: epidemiology,Influenza, Human: transmission,Influenza, Human: virology,Internationality,Internet,Internet: utilization,Linear Models,Office Visits,Office Visits: statistics {\&} numerical data,Population Surveillance,Population Surveillance: methods,Reproducibility of Results,Seasons,Time Factors,United States,User-Computer Interface},
number = {7232},
pages = {1012--4},
pmid = {19020500},
title = {{Detecting influenza epidemics using search engine query data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19020500},
volume = {457},
year = {2009}
}
@article{Tran2014,
abstract = {Big Data is identified by its three Vs, namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed re-cently addressing this emerging need. However, a hidden factor can represent an important fourth V, that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques catego-rized and detailed.},
author = {Tran, Dang-Hoan and Gaber, Mohamed Medhat and Sattler, Kai-Uwe},
doi = {10.1145/2674026.2674031},
file = {:Users/matt/Documents/Mendeley Desktop/Tran, Gaber, Sattler - 2014 - Change Detection in Streaming Data in the Era of Big Data Models and Issues.pdf:pdf},
isbn = {1931-0145},
issn = {1931-0145},
journal = {ACM SIGKDD Explorations Newsletter - Special issue on big data},
number = {1},
pages = {30--38},
title = {{Change Detection in Streaming Data in the Era of Big Data: Models and Issues}},
year = {2014}
}
@article{Kifer2004,
abstract = {Detecting changes in a data stream is an im- portant area of research with many appli- cations. In this paper, we present a novel method for the detection and estimation of change. In addition to providing statisti- cal guarantees on the reliability of detected changes, our method also provides meaning- ful descriptions and quantification of these changes. Our approach assumes that the points in the stream are independently gen- erated, but otherwise makes no assumptions on the nature of the generating distribution. Thus our techniques work for both continuous and discrete data. In an experimental study we demonstrate the power of our techniques.},
archivePrefix = {arXiv},
arxivId = {cond-mat/9310008},
author = {Kifer, Daniel and Ben-david, Shai and Gehrke, Johannes},
doi = {10.1016/0378-4371(94)90421-9},
eprint = {9310008},
file = {:Users/matt/Documents/Mendeley Desktop/Kifer, Ben-david, Gehrke - 2004 - Detecting Change in Data Streams.PDF:PDF},
isbn = {0120884690},
issn = {00394564},
journal = {Proceedings of the 30th VLDB Conference},
pages = {180--191},
pmid = {1631915},
primaryClass = {cond-mat},
title = {{Detecting Change in Data Streams}},
year = {2004}
}
@article{Xu2011,
author = {Xu, Yi and Zhang, Zhongfei and Yu, Philips and Long, Bo},
doi = {10.1145/2063576.2063735},
file = {:Users/matt/Documents/Mendeley Desktop/Xu et al. - 2011 - Pattern change discovery between high dimensional data sets.pdf:pdf},
isbn = {9781450307178},
journal = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
keywords = {ma-,mapping,pal angles,pattern change detection,princi-,principle of dominant subspace,unsupervised learning},
pages = {1097},
title = {{Pattern change discovery between high dimensional data sets}},
url = {http://dl.acm.org/citation.cfm?doid=2063576.2063735},
year = {2011}
}
@article{Rand1971,
author = {Rand, William M.},
doi = {10.1080/01621459.1971.10482356},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {dec},
number = {336},
pages = {846--850},
title = {{Objective Criteria for the Evaluation of Clustering Methods}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356},
volume = {66},
year = {1971}
}
@unpublished{Madrid2004,
author = {Galeano, Pedro and Pe{\~{n}}a, Daniel},
file = {:Users/matt/Documents/Mendeley Desktop/Madrid et al. - 2004 - Variance Changes Detection in Multivariate Time Series.pdf:pdf},
keywords = {heteroskedasticity,likelihood ratio test statistic,step changes,varma models},
title = {{Variance Changes Detection in Multivariate Time Series}},
year = {2004}
}
@article{Willsky1976,
abstract = {We consider a class of stochastic linear systems that are subject to jumps of unknown magnitudes in the state variables occurring at unknown times. This model can be used when considering such problems as estimation for systems subject to possible component failures and the tracking of vehicles capable of abrupt maneuvers. Using Kalman-Bucy filtering and generalized likelihood ratio techniques, we devise an adaptive filtering system for the detection and estimation of the jumps. An example that illustrates the dynamical properties of our filtering scheme is discusssed in detail.},
author = {Willsky, Alan S. and Jones, Harold L.},
doi = {10.1109/TAC.1976.1101146},
file = {:Users/matt/Documents/Mendeley Desktop/Willsky, Jones - 1976 - A Generalized Likelihood Ratio Approach to the Detection and Estimation of Jumps in Linear Systems.pdf:pdf},
isbn = {0018-9286},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {1},
pages = {108--112},
title = {{A Generalized Likelihood Ratio Approach to the Detection and Estimation of Jumps in Linear Systems}},
volume = {21},
year = {1976}
}
@article{Kent1955,
author = {Kent, Allen and Berry, Madeline M. and Luehrs, Fred U. and Perry, J. W.},
doi = {10.1002/asi.5090060209},
file = {:Users/matt/Documents/Mendeley Desktop/Kent et al. - 1955 - Machine literature searching VIII. Operational criteria for designing information retrieval systems.pdf:pdf},
issn = {0096946X},
journal = {American Documentation},
month = {apr},
number = {2},
pages = {93--101},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{Machine literature searching VIII. Operational criteria for designing information retrieval systems}},
url = {http://doi.wiley.com/10.1002/asi.5090060209},
volume = {6},
year = {1955}
}
@article{Qahtan2015,
abstract = {Detecting changes in multidimensional data streams is an important and challenging task. In unsupervised change detection, changes are usually detected by comparing the distribution in a current (test) window with a reference window. It is thus essential to design divergence metrics and density estimators for comparing the data distributions, which are mostly done for univariate data. Detecting changes in multidimensional data streams brings difficulties to the density estimation and comparisons. In this paper, we propose a framework for detecting changes in multidimensional data streams based on principal component analysis, which is used for project-ing data into a lower dimensional space, thus facilitating density estimation and change-score calculations. The proposed frame-work also has advantages over existing approaches by reducing computational costs with an efficient density estimator, promoting the change-score calculation by introducing effective divergence metrics, and by minimizing the efforts required from users on the threshold parameter setting by using the Page-Hinkley test. The evaluation results on synthetic and real data show that our frame-work outperforms two baseline methods in terms of both detection accuracy and computational costs.},
author = {Qahtan, Abdulhakim A. and Alharbi, Basma and Wang, Suojin and Zhang, Xiangliang},
doi = {10.1145/2783258.2783359},
file = {:Users/matt/Documents/Mendeley Desktop/Qahtan et al. - 2015 - A PCA-Based Change Detection Framework for Multidimensional Data Streams.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
keywords = {change detection,data streams,density estimation,principal component analysis},
pages = {935--944},
title = {{A PCA-Based Change Detection Framework for Multidimensional Data Streams}},
url = {http://dl.acm.org/citation.cfm?id=2783258.2783359},
year = {2015}
}
@inproceedings{Buntain2014,
author = {Buntain, Cody and Natoli, Christopher and Zivkovic, Miroslav},
booktitle = {Supercomputing},
file = {:Users/matt/Documents/Mendeley Desktop/Buntain, Natoli - Unknown - A Brief Comparison of Algorithms for Detecting Change Points in Data.pdf:pdf},
title = {{A Brief Comparison of Algorithms for Detecting Change Points in Data}},
url = {https://github.com/cbuntain/ChangePointDetection},
year = {2014}
}
@article{Braun2000,
abstract = {We consider situations where a step function with a variable number of steps provides an adequate model for a regression relationship, while the variance of the observations depends on their mean. This model provides for discontinuous jumps at changepoints and for constant means and error variances in between changepoints. The basic statistical problem consists of identification of the number of changepoints, their locations and the levels the function assumes in between. We embed this problem into a quasilikelihood formulation and utilise the minimum deviance criterion to fit the model; for the choice of the number of changepoints, we discuss a modified Schwarz criterion. A dynamic program-ming algorithm makes the segmentation feasible for sequences of moderate length. The performance of the segmentation method is demonstrated in an application to the segmen-tation of the Bacteriophage l sequence.},
author = {Braun, J V and Braun, R K and M{\"{u}}ller, H-G},
doi = {10.1093/biomet/87.2.301},
issn = {00063444},
journal = {Biometrika},
keywords = {Bacteriophage lambda,Deviance,Generalised linear model,Model selection,Schwarz criterion,Some key words,Step function},
month = {jun},
number = {2},
pages = {301--314},
title = {{Multiple changepoint fitting via quasilikelihood, with application to DNA sequence segmentation}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/87.2.301},
volume = {87},
year = {2000}
}
@article{Alvanaki2011,
abstract = {1 Hila Becker, Mor Naaman, and Luis Gravano. Learning similarity metrics for event identification in social media. WSDM, 2010. 2 Manish Bhide, Venkatesan T. Chakaravarthy, Krithi Ramamritham, and Prasan Roy. Keyword search over dynamic categorized information. ICDE, 2009.},
author = {Alvanaki, Foteini and Michel, Sebastian and Ramamritham, Krithi and Weikum, Gerhard},
doi = {10.1145/1989323.1989473},
file = {:Users/matt/Documents/Mendeley Desktop/Alvanaki et al. - 2011 - EnBlogue emergent topic detection in Web 2.0 streams.pdf:pdf},
isbn = {9781450306614},
issn = {07308078},
journal = {Proc. ACM SIGMOD International Conference on Management of Data},
keywords = {Social networks,Web},
pages = {1271--1274},
title = {{EnBlogue: emergent topic detection in Web 2.0 streams}},
url = {http://doi.acm.org/10.1145/1989323.1989473{\%}5Cnpapers2://publication/uuid/44E009D1-8574-49C1-A0AC-C2B247FE9043},
year = {2011}
}
@misc{Hromic2016,
author = {Hromic, Hugo},
booktitle = {GitHub repository},
publisher = {GitHub},
title = {python-bcubed},
url = {https://github.com/hhromic/python-bcubed},
year = {2016}
}
@article{Akaike1974,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
eprint = {arXiv:1011.1669v3},
file = {:Users/matt/Documents/Mendeley Desktop/Akaike - 1974 - A New Look at the Statistical Model Identification.pdf:pdf},
isbn = {0018-9286 VO - 19},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
pmid = {1100705},
title = {{A New Look at the Statistical Model Identification}},
volume = {19},
year = {1974}
}
@manual{RCoreTeam2017,
abstract = {R Development Core Team (2011). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org/.},
address = {Vienna, Austria},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{R Core Development Team}},
booktitle = {Document freely available on the internet at: http://www. r-project. org},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {3-900051-07-0},
issn = {1098-6596},
organization = {R Foundation for Statistical Computing},
pmid = {25246403},
title = {{R: a language and environment for statistical computing, 3.2.1}},
url = {https://www.r-project.org/},
year = {2015}
}
@article{Tartakovsky2005,
abstract = {An efficient sequential nonparametric multi- chart (multichannel) CUSUM-type detection test for detecting changes in multichannel sensor systems is proposed. While there is a wide spectrum of applications where it is necessary to consider multichannel generalizations and general statistical models in change-point detection problems, the study in this pa- per is motivated by network security. Many kinds of intrusions in computer networks lead to abrupt changes in network traffic. These changes have to be detected as rapidly as possible while maintaining a false alarm rate at a low level. Computer intru- sion detection encourages the development of a nonparametric multichannel change-point detection test that does not use exact legitimate (pre-change) and attack (post-change) traffic models. The proposed nonparametric detection procedure can be effec- tively applied to detect a wide variety of attacks such as exter- nal denial of service attacks, worm based attacks, port scanning, and insider man-in-the-middle attacks. Operating characteris- tics of the proposed multichannel CUSUMtest are evaluated for real denial of service attacks using traces recently collected by CAIDA. The results of a comparison with a conventional single- channel CUSUM algorithm show that the multichannel test has much better performance.},
author = {Tartakovsky, Alexander G and Rozovskii, Boris L},
file = {:Users/matt/Documents/Mendeley Desktop/Tartakovsky, Rozovskii - 2005 - A Nonparametric Multichart CUSUM Test for Rapid Intrusion Detection.pdf:pdf},
journal = {Proceedings of Joint Statistical Meetings},
keywords = {change detection,computer intrusion detection,denial of service attacks,multichart cusum tests},
pages = {7--11},
title = {{A Nonparametric Multichart CUSUM Test for Rapid Intrusion Detection}},
year = {2005}
}
@article{Killick2011,
abstract = {We consider the problem of detecting multiple changepoints in large oceanographic data sets. In this setting the amount of data being collected is continually increasing and consequently the number of changepoints will also increase with time. An efficient and accurate analysis of such data is of considerable interest to those working in the energy sector as understanding the characteristics of the ocean environment is central to reliable design and operation of marine and coastal structures. Detecting the presence of changepoints in oceanographic time-series is of particular importance, since statistical and engineering modelling of the ocean environment, structural loading and response typically assumes stationarity of the environment (in time). Drawing on recent work on efficient search methods by Killick et al. (2011), we compare and contrast the eff{\_}ect of diff{\_}erent approaches to this data, focusing in particular on computational and statistical aspects. The talk will conclude by highlighting the importance of such computationally efficient methods in an oceanographic setting.},
author = {Killick, Rebecca and Eckley, Idris and Jonathan, Philip},
file = {::},
keywords = {QA Mathematics},
pages = {4137--4142},
title = {{Efficient Detection Of Multiple Changepoints Within An Oceanographic Time Series}},
url = {http://www.lancaster.ac.uk/{~}jonathan/2011{\_}ISI{\_}ChangePoints{\_}Talk.pdf http://eprints.lancs.ac.uk/56978/},
year = {2011}
}
@article{Kulldorff2005,
abstract = {BACKGROUND: The ability to detect disease outbreaks early is important in order to minimize morbidity and mortality through timely implementation of disease prevention and control measures. Many national, state, and local health departments are launching disease surveillance systems with daily analyses of hospital emergency department visits, ambulance dispatch calls, or pharmacy sales for which population-at-risk information is unavailable or irrelevant. METHODS AND FINDINGS: We propose a prospective space-time permutation scan statistic for the early detection of disease outbreaks that uses only case numbers, with no need for population-at-risk data. It makes minimal assumptions about the time, geographical location, or size of the outbreak, and it adjusts for natural purely spatial and purely temporal variation. The new method was evaluated using daily analyses of hospital emergency department visits in New York City. Four of the five strongest signals were likely local precursors to citywide outbreaks due to rotavirus, norovirus, and influenza. The number of false signals was at most modest. CONCLUSION: If such results hold up over longer study times and in other locations, the space-time permutation scan statistic will be an important tool for local and national health departments that are setting up early disease detection surveillance systems.},
author = {Kulldorff, Martin and Heffernan, Richard and Hartman, Jessica and Assun????o, Renato and Mostashari, Farzad},
doi = {10.1371/journal.pmed.0020059},
file = {:Users/matt/Documents/Mendeley Desktop/Kulldorff et al. - 2005 - A space-time permutation scan statistic for disease outbreak detection.PDF:PDF},
isbn = {1549-1676 (Electronic)$\backslash$n1549-1277 (Linking)},
issn = {15491277},
journal = {PLoS Medicine},
number = {3},
pages = {0216--0224},
pmid = {15719066},
title = {{A space-time permutation scan statistic for disease outbreak detection}},
volume = {2},
year = {2005}
}
@article{Auger1989,
abstract = {Two algorithms for the efficient identification of segment neighborhoods are presented. A segment neighborhood is a set of contiguous residues that share common features. Two procedures are developed to efficiently find estimates for the parameters of the model that describe these features and for the residues that define the boundaries of each segment neighborhood. The algorithms can accept nearly any model of segment neighborhood, and can be applied with a broad class of best fit functions including least squares and maximum likelihood. The algorithms successively identify the most important features of the sequence. The application of one of these methods to the haemagglutinin protein of influenza virus reveals a possible mechanism for conformational change through the finding of a break in a strong heptad repeat structure.},
author = {Auger, Ivan E. and Lawrence, Charles E.},
doi = {10.1007/BF02458835},
file = {::},
isbn = {0092-8240 (Print)$\backslash$r0092-8240 (Linking)},
issn = {00928240},
journal = {Bulletin of Mathematical Biology},
month = {jan},
number = {1},
pages = {39--54},
pmid = {2706400},
publisher = {Kluwer Academic Publishers},
title = {{Algorithms for the optimal identification of segment neighborhoods}},
url = {http://link.springer.com/10.1007/BF02458835},
volume = {51},
year = {1989}
}
@article{Journal2009,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY It is shown that a strongly consistent estimation procedure for the order of an auto-regression can be based on the law of the iterated logarithm for the partial auto-correlations. As compared to other strongly consistent procedures this procedure will},
author = {Hannan, E J and Quinn, B G},
doi = {10.2307/2985032},
file = {:Users/matt/Documents/Mendeley Desktop/Hannan, Quinn - 1979 - The Determination of the Order of an Autoregression.pdf:pdf},
isbn = {00359246},
issn = {00359246},
journal = {Journal of the Royal Statistical Society},
keywords = {aic,autoregression,autoregressive order,law of iterated logarithm},
number = {2},
pages = {190--195},
title = {{The Determination of the Order of an Autoregression}},
url = {http://www.jstor.org/stable/2985032},
volume = {41},
year = {1979}
}
@manual{Chen2011,
author = {Chen and W.-C.},
booktitle = {Ph.D. Diss., Iowa Stat University},
title = {{Overlapping Codon Model, Phylogenetic Clustering, and Alternative Partial Expectation Conditional Maximization Algorithm}},
url = {http://gradworks.umi.com/34/73/3473002.html},
year = {2011}
}
@article{ThomasJ.McCabe1976,
abstract = {This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program complexity. The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the correlation between intuitive complexity and the graph-theoretic complexity. Several properties of the graph-theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program.},
author = {McCabe, Thomas J.},
file = {::},
journal = {IEEE Trans. Softw. Eng.},
keywords = {Metrics,complexity measure,control flow,decomposition,graph theory,independence,linear,modularization,programming,reduction,software testing},
number = {4},
pages = {308--320},
title = {{A Complexity Measure}},
url = {http://juacompe.mrchoke.com/natty/thesis/FrameworkComparison/A complexity measure.pdf},
volume = {2},
year = {1976}
}
@article{Siegmund1995,
abstract = {We study sequential detection of a change-point using the generalized likelihood ratio statistic. For the special case of detecting a change in a normal mean with known variance, we give approximations to the average run lengths and compare our procedure to standard CUSUM tests and combined CUSUM-Shewhart tests. Several examples indicating extensions to problems involving multiple parameters are discussed.},
author = {Siegmund, D. and Venkatraman, E. S.},
doi = {10.1214/aos/1176324466},
file = {:Users/matt/Documents/Mendeley Desktop/Siegmund, Venkatraman - 1995 - Using the generalized likelihood ratio statistic for sequential detection of a change-point.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {1},
pages = {255--271},
title = {{Using the generalized likelihood ratio statistic for sequential detection of a change-point}},
volume = {23},
year = {1995}
}
@article{Haynes2014,
abstract = {In the multiple changepoint setting, various search methods have been proposed which involve optimising either a constrained or penalised cost function over possible numbers and locations of changepoints using dynamic programming. Such methods are typically computationally intensive. Recent work in the penalised optimisation setting has focussed on developing a pruning-based approach which gives an improved computational cost that, under certain conditions, is linear in the number of data points. Such an approach naturally requires the specification of a penalty to avoid under/over-fitting. Work has been undertaken to identify the appropriate penalty choice for data generating processes with known distributional form, but in many applications the model assumed for the data is not correct and these penalty choices are not always appropriate. Consequently it is desirable to have an approach that enables us to compare segmentations for different choices of penalty. To this end we present a method to obtain optimal changepoint segmentations of data sequences for all penalty values across a continuous range. This permits an evaluation of the various segmentations to identify a suitably parsimonious penalty choice. The computational complexity of this approach can be linear in the number of data points and linear in the difference between the number of changepoints in the optimal segmentations for the smallest and largest penalty values. This can be orders of magnitude faster than alternative approaches that find optimal segmentations for a range of the number of changepoints.},
archivePrefix = {arXiv},
arxivId = {1412.3617},
author = {Haynes, Kaylea and Eckley, Idris A. and Fearnhead, Paul},
eprint = {1412.3617},
file = {:Users/matt/Documents/Mendeley Desktop/Haynes, Eckley, Fearnhead - 2014 - Efficient penalty search for multiple changepoint problems.pdf:pdf},
keywords = {dynamic programming,penalised likelihood,segmenta-,structural change},
pages = {1--23},
title = {{Efficient penalty search for multiple changepoint problems}},
url = {http://arxiv.org/abs/1412.3617},
year = {2014}
}
@book{Wickham2009,
abstract = {This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkison's Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, it's easy to:produce handsome, publication-quality plots, with automatic legends created from the plot specificationsuperpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scalesadd customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regressionsave any ggplot2 plot (or part thereof) for later modification or reusecreate custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plotsapproach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot.This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, to and you'll find it easy to get graphics out of your head and on to the screen or page.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wickham, Hadley},
booktitle = {Media},
doi = {10.1007/978-0-387-98141-3},
eprint = {arXiv:1011.1669v3},
isbn = {9780387981406},
issn = {0006341X},
number = {July},
pages = {211},
pmid = {19791908},
publisher = {Springer-Verlag New York},
title = {{Elegant Graphics for Data Analysis}},
url = {http://had.co.nz/ggplot2/book},
volume = {35},
year = {2009}
}
@manual{Bellosta2015,
abstract = {Description Run Python code, make function calls, assign and retrieve variables, etc. from R. Depends RJSONIO ({\textgreater}= 0.7-3) License GPL-2 SystemRequirements Python ({\textgreater}= 2.7) and Python headers and libraries (See the INSTALL file) OS{\_}type unix URL http://rpython.r-forge.r-project.org/ NeedsCompilation yes},
annote = {R package version 0.0-6},
author = {Gil, Carlos J and Maintainer, Bellosta and Bellosta, Carlos J Gil},
title = {{'rPython' Package Allowing R to Call Python}},
url = {https://cran.r-project.org/web/packages/rPython/rPython.pdf},
year = {2015}
}
@article{Fawcett1999,
abstract = {We introduce a problem class which we term activity monitoring. Such problems involve monitoring the behavior of a large population of entities for interesting events requiring action. We present a framework within which each of the individual problems has a natural expression, as well as a methodology for evaluating performance of activity monitoring techniques. We show that two superficially different tasks, news story monitoring and intrusion detection, can be expressed naturally within the...},
author = {Fawcett, Tom and Provost, Foster},
doi = {10.1016/j.ecoleng.2010.11.031},
file = {:Users/matt/Documents/Mendeley Desktop/Fawcett, Provost - 1999 - Activity monitoring Noticing interesting changes in behavior.pdf:pdf},
isbn = {1581131437},
issn = {09258574},
journal = {Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
number = {212},
pages = {53--62},
title = {{Activity monitoring: Noticing interesting changes in behavior}},
url = {http://portal.acm.org/citation.cfm?id=312195},
volume = {1},
year = {1999}
}
@article{Pelecanos2010,
abstract = {Detection of outbreaks is an important part of disease surveillance. Although many algorithms have been designed for detecting outbreaks, few have been specifically assessed against diseases that have distinct seasonal incidence patterns, such as those caused by vector-borne pathogens.},
author = {Pelecanos, Anita M and Ryan, Peter a and Gatton, Michelle L},
doi = {10.1186/1472-6947-10-74},
file = {:Users/matt/Documents/Mendeley Desktop/Pelecanos, Ryan, Gatton - 2010 - Outbreak detection algorithms for seasonal disease data a case study using Ross River virus disease.pdf:pdf},
issn = {1472-6947},
journal = {BMC medical informatics and decision making},
number = {1},
pages = {74},
pmid = {21106104},
title = {{Outbreak detection algorithms for seasonal disease data: a case study using Ross River virus disease.}},
url = {http://www.biomedcentral.com/1472-6947/10/74},
volume = {10},
year = {2010}
}
@article{Page1954,
abstract = {In this study, the effect of powder cellulose (PC) and 2 types of microcrystalline cellulose (MCC 101 and MCC 301) on pellet properties produced by an extrusion/spheronization process was investigated. The different investigated types of cellulose displayed different behavior during the extrusion/spheronization process. Pure PC was unsuitable for extrusion, because too much water was required and the added water was partly squeezed during the extrusion process. In contrast, MCC 101 and MCC 301 were extrudable at a wide range of water content, but the quality of the resulting products varied. In the extrusion/spheronization process, MCC 101 was the best substance, with easy handling and acceptable product properties. The properties of the extrudates and pellets were determined by Fourier transform (FT) Raman spectroscopy and environmental scanning electron microscopy (ESEM). FT-Raman spectroscopy was able to distinguish between the original substances and also between the wet and dried extrudates. The particle sizes of the raw material and of the extrudates were determined by ESEM without additional preparation. For MCC, the size of the resulting particles within the extrudate or pellet was smaller. However, in the extrudates of PC, changes in particle size could not be observed. http://www.jstor.org/stable/2333009?origin=crossref},
author = {Page, E. S.},
doi = {10.2307/2333009},
file = {:Users/matt/Documents/Mendeley Desktop/Page - 1954 - Continuous Inspection Schemes(2).pdf:pdf},
isbn = {00063444},
issn = {00063444},
journal = {Biometrika},
month = {jun},
number = {1/2},
pages = {100},
publisher = {Oxford University Press},
title = {{Continuous Inspection Schemes}},
url = {http://www.jstor.org/stable/2333009?origin=crossref},
volume = {41},
year = {1954}
}
@article{Matteson2012,
abstract = {Change point analysis has applications in a wide variety of fields. The general problem concerns the inference of a change in distribution for a set of time-ordered observations. Sequential detection is an online version in which new data is continually arriving and is analyzed adaptively. We are concerned with the related, but distinct, offline version, in which retrospective analysis of an entire sequence is performed. For a set of multivariate observations of arbitrary dimension, we consider nonparametric estimation of both the number of change points and the positions at which they occur. We do not make any assumptions regarding the nature of the change in distribution or any distribution assumptions beyond the existence of the $\alpha$th absolute moment, for some $\alpha$ ∈ (0,2). Estimation is based on hierarchical clustering and we propose both divisive and agglomerative algorithms. The divisive method is shown to provide consistent estimates of both the number and location of change points under standard regularity assumptions. We compare the proposed approach with competing methods in a simulation study. Methods from cluster analysis are applied to assess performance and to allow simple comparisons of location estimates, even when the estimated number differs. We conclude with applications in genetics, finance and spatio-temporal analysis.},
archivePrefix = {arXiv},
arxivId = {1306.4933},
author = {Matteson, David S. and James, Nicholas A.},
doi = {10.1080/01621459.2013.849605},
eprint = {1306.4933},
file = {:Users/matt/Documents/Mendeley Desktop/Matteson, James - 2012 - A nonparametric approach for multiple change point analysis of multivariate data.pdf:pdf},
issn = {0162-1459},
journal = {Submitted},
keywords = {cluster analysis,multivariate time series,permutation tests,signal processing},
pages = {1--29},
title = {{A nonparametric approach for multiple change point analysis of multivariate data}},
volume = {14853},
year = {2012}
}
@article{Bersimis2007,
abstract = {In this paper we discuss the basic procedures for the implementation of multivariate statistical process control via control charting. Furthermore, we review multivariate extensions for all kinds of univariate control charts, such as multivariate Shewhart- type control charts, multivariate CUSUM control charts and multivariate EWMA control charts. In addition, we review unique procedures for the construction of multivariate control charts, based on multivariate statistical techniques such as principal components analysis (PCA) and partial least squares (PLS). Finally, we describe the most significant methods for the interpretation of an out-of-control signal.},
author = {Bersimis, S. and Psarakis, S. and Panaretos, J.},
doi = {10.1002/qre.829},
file = {:Users/matt/Documents/Mendeley Desktop/Bersimis, Psarakis, Panaretos - 2007 - Multivariate statistical process control charts An overview.pdf:pdf},
isbn = {0748-8017},
issn = {07488017},
journal = {Quality and Reliability Engineering International},
keywords = {CUSUM,EWMA,Hotelling's T2,Multivariate statistical process control,PCA,PLS,Process control,Quality control},
number = {5},
pages = {517--543},
title = {{Multivariate statistical process control charts: An overview}},
volume = {23},
year = {2007}
}
@article{Sing2005,
author = {Sing, T. and Sander, O. and Beerenwinkel, N. and Lengauer, T.},
doi = {10.1093/bioinformatics/bti623},
file = {:Users/matt/Documents/Mendeley Desktop/Sing et al. - 2005 - ROCR visualizing classifier performance in R.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {oct},
number = {20},
pages = {3940--3941},
publisher = {Oxford University Press},
title = {{ROCR: visualizing classifier performance in R}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bti623},
volume = {21},
year = {2005}
}
@article{Jackson2003,
abstract = {Many signal processing problems can be solved by maximizing the fitness of a segmented model over all possible partitions of the data interval. This letter describes a simple but powerful algorithm that searches the exponentially large space of partitions of {\$}N{\$} data points in time {\$}O(N{\^{}}2){\$}. The algorithm is guaranteed to find the exact global optimum, automatically determines the model order (the number of segments), has a convenient real-time mode, can be extended to higher dimensional data spaces, and solves a surprising variety of problems in signal detection and characterization, density estimation, cluster analysis and classification.},
archivePrefix = {arXiv},
arxivId = {math/0309285},
author = {Jackson, Brad and Scargle, Jeffrey D. and Barnes, David and Arabhi, Sundararajan and Alt, Alina and Gioumousis, Peter and Gwin, Elyus and Sangtrakulcharoen, Paungkaew and Tan, Linda and Tsai, Tun Tao},
doi = {10.1109/LSP.2001.838216},
eprint = {0309285},
file = {::},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Bayesian modeling,Cluster analysis,Density estimation,Histograms,Optimization,Signal detection},
month = {sep},
number = {2},
pages = {105--108},
primaryClass = {math},
title = {{An algorithm for optimal partitioning of data on an interval}},
url = {http://arxiv.org/abs/math/0309285 http://dx.doi.org/10.1109/LSP.2001.838216},
volume = {12},
year = {2005}
}
@article{Downey2008,
abstract = {We propose an algorithm for simultaneously detecting and locating changepoints in a time series, and a framework for predicting the distribution of the next point in the series. The kernel of the algorithm is a system of equations that computes, for each index i, the probability that the last (most recent) change point occurred at i. We evaluate this algorithm by applying it to the change point detection problem and comparing it to the generalized likelihood ratio (GLR) algorithm. We find that our algorithm is as good as GLR, or better, over a wide range of scenarios, and that the advantage increases as the signal-to-noise ratio decreases.},
archivePrefix = {arXiv},
arxivId = {0812.1237},
author = {Downey, Allen B.},
eprint = {0812.1237},
file = {:Users/matt/Documents/Mendeley Desktop/Downey - 2008 - A novel changepoint detection algorithm.pdf:pdf},
journal = {Applied Microbiology and Biotechnology},
pages = {1--11},
title = {{A novel changepoint detection algorithm}},
url = {http://arxiv.org/abs/0812.1237},
year = {2008}
}
@article{Dasu2009,
abstract = {Data streams are dynamic, with frequent distributional changes. In this paper, we propose a statistical approach to detecting distributional shifts in multi-dimensional data streams. We use relative entropy, also known as the Kullback-Leibler distance, to measure the statistical distance between two distributions. In the context of a multi-dimensional data stream, the distributions are generated by data from two sliding windows. We maintain a sample of the data from the stream inside the windows to build the distributions. Our algorithm is streaming, nonparametric, and requires no distributional or model assumptions. It employs the statistical theory of hypothesis testing and bootstrapping to determine whether the distributions are statistically different. We provide a full suite of experiments on synthetic data to validate the method and demonstrate its effectiveness on data from real-life applications. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Dasu, Tamraparni and Krishnan, Shankar and Lin, Dongyu and Venkatasubramanian, Suresh and Yi, Kevin},
doi = {10.1007/978-3-642-03915-7_3},
file = {:Users/matt/Documents/Mendeley Desktop/Dasu et al. - 2009 - Change (detection) you can believe in Finding distributional shifts in data streams.pdf:pdf},
isbn = {3642039146},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {21--34},
title = {{Change (detection) you can believe in: Finding distributional shifts in data streams}},
volume = {5772 LCNS},
year = {2009}
}
