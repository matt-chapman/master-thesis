\documentclass[../main.tex]{subfiles}

\begin{document}

When asserting the veracity of an approach to a particular problem in software engineering, this is almost always achieved by utilising some sort of metric or measure. To give an example, when evaluating the quality of some new piece of software, this could be done through calculating measures such as McCabe Complexity \cite{ThomasJ.McCabe1976} or unit test coverage.

The problem of detecting change points in a data stream or time series is no different. There exists within this field a number of metrics that may be used to prove the accuracy and effectiveness of a given approach, and these often fall into one of three categories: binary classification (e.g. F1 score), clustering (e.g. BCubed, or the Rand Index), or information retrieval (utilising scoring systems such as those utilised in the TREC 2016 Real Time Summarisation Track \cite{trec2016}).

Being that there are myriad ways to detect change points in a data stream, data set, or time-series, it is necessary to implement the application of some evaluation measure to back up an assertion that, for example, approach \emph{A} is better than approach \emph{B} for some data set. The application of these measures can, in some cases, result in more questions than answers, especially in situations where two different measures may disagree with the results of the aforementioned approaches \emph{A} and \emph{B}.

The purpose of this thesis is twofold: firstly, to conduct a meta analysis of evaluation measures, and how they perform in the domain of change point detection problems. Being that these measures are metrics designed for other problems that are not necessarily change point detection, it stands to reason that there are perhaps situations where families of metrics will disagree with each-other, or disagree with by-eye evaluation by domain experts. The second purpose is to evaluate the veracity of a number of existing change point detection algorithms when they are applied to data from social media.

For the purposes of transparency and to support the idea of `open science', the full source code written for the experiments in this thesis, as well as the CSV files containing raw data before processing and the experiment results are made available in a Git repository hosted online. The full \LaTeX source for this document is also made available in this repository.\footnote{\url{https://github.com/matt-chapman/master-thesis}}

\section{Problem Statement}
\label{Problem Statement}

For the number of change point detection methods that exist, there is an almost equal number of ways to evaluate the results of the methods. There is little agreement between researchers on the `correct' method to use, and as this research will show, there are problems that exist when utilising measures that have generally been widely accepted for this purpose.

Outside of the `normal' applications of change point detection (for example, spotting the onset of `storm seasons' in oceanographic data \cite{Killick2011} or the detection of past changes in the valuation of a currency, such as Bitcoin \cite{Buntain2014}), there is also an application for change point detection as an `event detection' mechanism, when applied to data such as that sourced from online social media platforms.

There exists methods of event detection in social media data utilising approaches such as term frequency counting, lexicon-based recognition and context-free-grammar algorithms. However, these approaches rely on (sometimes computationally expensive) analysis of the content of messages being posted on social media platforms (see, for example, \citeauthor{Alvanaki2011} \cite{Alvanaki2011}). Being that change point detection algorithms, (and especially \emph{online} change point detection algorithms - that is, change point detection approaches that operate on `streaming data': data that has values constantly appended to it) can be utilised for the detection of past events, it stands to reason that these algorithms can also be utilised for event detection when applied to pre-computed data such as conversation volume or reach of a particular conversation. There certainly exists a requirement in the field of online reputation management to be able to inform businesses (in a timely manner) that a spike in conversation volume is occurring, and thus they may need to carry out some action to mitigate reputational damage in the case of negative sentiment. Therefore, this thesis intends to answer the following research question, from which a number of sub-questions have been formulated:

\begin{description}
    \item[RQ] Are existing metrics in the field of change point detection effective and accurate?
    \begin{description}
    \item[SQ1] In what way are existing metrics deficient when applied to change point detection problems?
    \item[SQ2] Do existing metrics agree on the `best' approach when used to evaluate change point detection algorithms applied to real-world data?
    \item[SQ3] Is there a metric more suited than the others, for the purpose of evaluating change point detections according to functional requirements set forth by the host company?
    \item[SQ4] What would an ideal metric for evaluating change point detection approaches look like?
    \item[SQ5] Do metrics show that change point detection is a reasonable and effective approach for the use-case of the host organisation?
\end{description}
\end{description}

\section{Motivation}

This particular research is motivated specifically by the online reputation management sector. The business hosting this research project (Buzzcapture B.V.\footnote{\url{http://www.buzzcapture.com}}) is a Dutch consultancy that provides online reputation management services to many other businesses throughout Europe. Recently acquired by Obi Group B.V\footnote{\url{https://www.obi4wan.com/}}, Buzzcapture is one of the largest and most widely engaged online reputation and webcare companies in Europe. Chief among the provided services is the BrandMonitor application, which, among other features, provides a rudimentary notification system for clients that is triggered once there is an absolute or relative increase in conversation volume (conversation volume being defined as the number of online messages or postings relevant to the client over a given time period).

Buzzcapture made a project available to students of the Universiteit van Amsterdam, wherein they would provide a method for more effectively providing these notifications, based on more than just an arbitrary threshold on conversation volume or some other computed metric. Upon accepting this project, research was carried out into the field of change detection algorithms, during which it was found that there was not a \emph{single} accepted approach for evaluating measures.

Indeed, for every publication that described some novel change point detection algorithm, there was a slightly different approach for evaluating it, and proving the veracity of algorithm in a certain situation. Most publications made use of some sort of binary classification measure (for example, \citeauthor{Qahtan2015} \cite{Qahtan2015}, \citeauthor{Buntain2014} \cite{Buntain2014}, and \citeauthor{Pelecanos2010} \cite{Pelecanos2010}), while others had small variations on that theme, providing additional takes on the binary classification approach using methods such as \emph{Receiver Operating Characteristic} curves (for example \citeauthor{Fawcett1999} \cite{Fawcett1999} \& \citeauthor{Desobry2005} \cite{Desobry2005}). Additionally, there were publications that made use of clustering measures, calculated using a segmentation of the time series based on computed change points, such as that published by \citeauthor{Matteson2012} \cite{Matteson2012}.

It is the intention of this thesis to not only answer the research questions set out in \autoref{Problem Statement}, but also to provide a robust recommendation of a change detection methodology which could then eventually be implemented into the Brandmonitor tool to supply timely and relevant notifications to clients when a conversation concerning their brand exhibits a change in behaviour or otherwise `goes viral'.

\section{Document Structure}

This thesis document is split into a number of logical `chapters', each focussing on a specific aspect of the work carried out.

\begin{description}
    \item[Background] Explanatory notes regarding the change point detection methods being utilised in this thesis (including an explanation of critical values such as penalty scores), as well as explanations of the evaluation methods being utilised in this thesis and how they are calculated.
    \item[Research Method] A summary of how the research conducted in this thesis was carried out. This chapter contains a listing of the experiments that took place and explains how they were conducted.
    \item[Research] A factual summary of findings for each experiment, including an answer as to which hypothesis held. This section does not contain a discussion of the results, rather preferring to concentrate on the facts as they appear.
    \item[Results] A discussion of the experiment results, as well as the conclusions that could be gleaned from them.
    \item[Results Discussion] A summary of answers for each of the research questions, including a brief discussion concerning any threats to validity for the project.
    \item[Conclusions] A full summary of the context of this work, and the conclusions that this thesis contributes to the field.
    \item[Future Work] A brief discussion of future work that could and should be carried out in order to further explore the results that this thesis contributes to the field. This includes work that can be carried out to further prove the results of the experiments carried out herein, as well as work that would build upon these results and provide additional insights and conclusions.
\end{description}

\end{document}