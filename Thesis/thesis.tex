%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{uvamscse}	% UvA thesis class

% numeric citations, name/title/year sorting
%\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, sorting=nty]{biblatex}

\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage[sorting=nty,style=numeric,backend=biber]{biblatex}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\mmark}{\ding{108}}%

\usetikzlibrary{positioning}

% program listings environment, see uvamscse.cls
\input{program-listings}
\newcommand{\cmd}[1]{\texttt{$\backslash$#1}}

\title{Detecting Online Conversations Going Viral}
\coverpic[250pt]{figures/Buzzcapture_thunder.png}
\subtitle{Evaluation of change point detection methods for time-series data}
\date{Spring 2017}

\author{Matt Chapman}
\authemail{matthew.chapman@student.uva.nl}
\host{Buzzcapture International, \url{http://www.buzzcapture.com}}
\supervisor{Evangelos Kanoulas, Universiteit van Amsterdam}

\abstract{
	Change point detection is a highly complex field that is hugely important for industries ranging from manufacturing and IT infrastructure fault detection, to online reputation management. There exists in this field a number of metrics or scores that are widely used for proving the veracity of new or novel approaches. However, there is little consensus as to which measure(s) are most effective in this field. This thesis carries out research into the field of change point detection, carrying out a comparative study of the behaviours of various scoring metrics using simulated data, the accuracy of change point detection methods using real world data, and makes a recommendation for the approach to be used for change point detection in the production systems of an online reputation management company. Finally, a short discussion on the ideal properties of a scoring metric from change point detection algorithms is carried out.
	
	In this study, it is found that many of the established scoring metrics behave inconsistently when applied to change point detection problems, and exhibit properties that bring their usefulness and accuracy into question. It is also found that despite these discovered issues, they nonetheless consistently agree on the most effective algorithm when used with real-world data as opposed to simulations.}

\newlength\mylen
\newcommand\myinput[1]{%
  \settowidth\mylen{\KwIn{}}%
  \setlength\hangindent{\mylen}%
  \hspace*{\mylen}#1\\}
  
\newcommand{\acite}[2]{
\noindent\cite{#1}~\parbox[t]{\linewidth-17.8pt}{\vspace{-.65em}\textbf{\fullcite{#1}}}\begin{quote}#2\end{quote}
}
  
\bibliography{../Bib/library.bib}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgments}

\chapter{Problem Statement \& Motivation}

\section{Problem Statement}
\label{Problem Statement}

When asserting the veracity of an approach to a particular problem in software engineering, this is almost always achieved by utilising some sort of metric or measure. To give an example, when evaluating the quality of some new piece of software, this could be done through calculating measures such as McCabe Complexity \cite{ThomasJ.McCabe1976} or unit test coverage.

The problem of detecting change points in a data stream is no different. There exists within this field a number of metrics that may be used to prove the accuracy and effectiveness of a given approach, and these often fall into one of three categories: binary classification (e.g. F1 score), clustering (e.g. BCubed, or the Rand Index), or information retrieval (utilising scoring systems such as those utilised in the TREC 2016 Real Time Summarisation Track: \cite{trec2016}).

Being that there are myriad ways to detect change points in a data stream, it is necessary to implement the application of some evaluation measure to back up an assertion that, for example, approach \emph{A} is better than approach \emph{B} for some data set $y_s$. The application of these measures can, in some cases, result in more questions than answers, especially in situations where two different measures may disagree with the results of the aforementioned approaches \emph{A} and \emph{B}.

However, for the number of change point detection methods that exist, there is an almost equal number of ways to evaluate the results of the methods. There is little agreement between researchers on the `correct' method to use, and as this research will show, there are problems that exist when utilising measures that have generally been widely accepted for this purpose.

Outside of the `normal' applications of change point detection (for example, spotting the onset of `storm seasons' in oceanographic data \cite{Killick2011} or the detection of past changes in the valuation of a currency, such as BitCoin \cite{Buntain2014}, there is also an application for change point detection as an `event detection' mechanism, when applied to data such as that sourced from online social media platforms.

There exists methods of event detection in social media data utilising approaches such as term frequency counting, lexicon-based recognition and context-free-grammar algorithms \todo{cite yordi}. However, these approaches rely on (sometimes computationally expensive) analysis of the content of messages being posted on social media platforms. Being that change point detection (and especially \emph{online} change point detection) algorithms can be utilised for the detection of past events, it stands to reason that these algorithms can also be utilised for event detection when applied to pre-computed data such as conversation volume or reach of a particular conversation. There certainly exists a requirement in the field of online reputation management to be able to inform businesses (in a timely manner) that a spike in conversation volume is occurring, and thus they may need to carry out some action to mitigate reputational damage if the conversation sentiment is negative.

Therefore, this thesis intends to answer the following research questions:

\begin{description}
	\item[RQ1] Are existing change point detection algorithms effective at detecting changes/events in social media data in a timely fashion?
	\item[RQ2] Are measures not specifically defined for the purpose of change detection evaluation effective in this domain?
	\item[RQ3] Do the aforementioned measures agree with by-eye analysis of results from change detection methods?
	\item[RQ4] If existing measures are deficient in some way, what would an `ideal' measure look like?\todo{reword}
\end{description}

From these research questions, the following sub-questions have been formulated:

\begin{description}
    \item[SQ1.1] Which algorithm, out of those being analysed, performs the `best' when applied against real world data?
    \item[SQ2.1] In what way do established measures fail to behave correctly?
    \item[SQ2.2] Which evaluation measure provides the best evaluation, given the functional requirements set forth by the host organisation?
\end{description}

\improvement[inline]{this needs expansion, I think}

\section{Motivation}

This particular research is motivated specifically by the online reputation management sector. The business hosting this research project (Buzzcapture b.v.\footnote{\url{http://www.buzzcapture.com}}) is a Dutch consultancy that provides online reputation management services to other businesses throughout Europe. Chief among these services is the BrandMonitor application, which, among other features, provides a rudimentary notification system for clients that is triggered once there is an absolute increase in conversation volume (conversation volume being defined as the number of tweets relevant to the client over a given time period).

Buzzcapture made a project available to students of the Universiteit van Amsterdam, wherein they would provide a method for more effectively providing these notifications, based on more than just an arbitrary threshold on conversation volume or some other computed metric. Upon accepting this project, research was carried out into the field of change detection algorithms, during which it was found that there was not a \emph{single} accepted approach for evaluating measures. Indeed, for every publication that described some novel change point detection algorithm, there was a slightly different approach for evaluating it, and proving the veracity of algorithm in a certain situation. Most publications made use of some sort of binary classification measure (for example, \cite{Qahtan2015}, \cite{Buntain2014}, \cite{Pelecanos2010}), while others had small variations on that theme, providing additional takes on the binary classification approach using methods such as \emph{Receiver Operating Characteristic} curves (for example \cite{Fawcett1999} \& \cite{Desobry2005}). Additionally, there were publications that made use of clustering measures, calculated using a segmentation of the time series based on computed change points, such as \cite{Matteson2012}.

It is the intention of this thesis to not only answer the research questions set out in \autoref{Problem Statement}, but also to provide a robust recommendation and working prototype of a change detection methodology which could then eventually be implemented into the Brand Monitor tool to supply timely and relevant notifications to clients when a conversation concerning their brand exhibits a change in behaviour or otherwise `goes viral'.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related Works}

What follows is an annotated bibliography of works that were read as part of the preparation for this research. It is not an exhaustive list, merely a set of papers, books and other publications which are relevant to this research or provided some insight into how best to carry it out. Full bibliographical citations are provided for each text.\bigskip

\acite{Buntain2014}{

One of the first papers on the subject read by the thesis author, this paper discusses and compares three different types of change detection algorithm: the \textit{Likelihood Ratio Test} (LRT), the \textit{Cumulative Sum} (CUSUM) test, and the \textit{Kernel-based Change Detection} (KCD) algorithm. The authors of this paper also kindly provided source code implementations of these algorithms, once contact was established. The algorithms were applied to several data sets, such as historical Bitcoin valuations and data from structural stress sensors.\bigskip}

\acite{Kawahara2009}{

This paper proposes what the authors call a ``novel non-parametric change detection algorithm''\cite{Kawahara2009}. It is applied alongside four other approaches, against three different data sets generated by models borrowed from other research papers on time-series data change detection. The approaches are evaluated according to accuracy rate and degree, though not for performance or other relevant metrics.\bigskip}

\acite{Kifer2004}{

This paper, much like the one written by Kawahara and Sugiyama, presents a novel change detection algorithm for evaluation. However, this particular paper differs in that the presented algorithm is also useful for estimation of the detected change. They also discuss the use of a `two window' approach to applying change detection algorithms to data streams, so as to limit memory usage in practice. This part in particular is relevant to the business case of the organisation hosting this thesis.\bigskip}

\acite{Madrid2004}{

This paper served as the basis of two of the algorithms tested in the paper written by \citeauthor{Buntain2014} This paper also provides it’s own evaluations of the approaches contained therein.\bigskip}

\acite{Desobry2005}{

Desobry, Davy, and Doncarli write concerning an implementation of a Kernel Change Detection (KCD) algorithm that is considered \textit{online} - that is, applicable to moving and updating data sets such as those that are processed by the host organisation of this thesis. This is particularly important, as any final implementation of a change detection method in the production systems of the thesis host company will be handling online data.

This paper makes use of Receiver Operating Characteristic curves to show the accuracy of change point detection algorithms, which is a variation on standard binary classification measures concerning false/true positives/negatives.\bigskip}

\acite{Tartakovsky2006}{

This paper discusses applying a CUSUM (Cumulative Sum) approach for detection of network intrusions. The approach taken by Tartakovsky et al. was intended to research the possibility of change detection while maintaining a low rate of false alarms. The idea was to apply an algorithm for this purpose that would not need to take into account pre-change and post-change models in order to be effective.\bigskip}

\acite{Tartakovsky2005}{

A collection of papers and discussions by the same authors as the above papers, expanding somewhat on the problem they tackled and the solutions found.\bigskip}

\acite{Matteson2012}{

Discussion of an `offline' (that is, applying an algorithm to a fixed data-set as opposed to processing moving `live' data) approach to change detection in multi-variate data. The authors carry out a simulation study to compare various approaches to this problem and present their results.\bigskip}

\acite{Siegmund1995}{

A paper on using a \textit{Generalized Likelihood Ratio} test for the detection of change points in a data set. An old piece of text that pre-dates social media, yet is still useful in explaining how the GLR test can be used for the detection of change points. The GLR approach is compared against standard CUSUM tests.\bigskip}

\acite{Willsky1976}{

Much like the paper written by Siegmund and Venkatraman, this paper pre-dates social media as a data source to which change detection algorithms could be applied. It is however, a useful look into how change detection algorithms have progressed and improved over the years. This paper is primarily focussed on uses of change detection in automated controls (being that it was published in the IEEE Transactions on Automatic Control), but is still a useful resource to understand how change detection algorithms can be evaluated.\bigskip}

\acite{Lai1999}{

Another paper concerning change point detection in control systems, Lai and Shan write primarily regarding Generalised Likelihood Ratio tests, and how these can be ‘configured’, specifically with regards to window size.\bigskip}

\acite{Bersimis2007}{

\noindent Control charts are a mechanism used in various industries to decide whether a given process is `in control' or `out of control'. In this way, control charts are used to discover outlying or anomalous results in a given data set, or inform operators of a sudden change in the data. This makes control charts particularly relevant to my research.

This particular paper discusses various approaches to control charts, and the methods behind their operation.\bigskip}

\acite{Downey2008}{
Downey discusses his creation of a `novel change detection algorithm' that can also be used for ``\dots predicting the distribution of the next point in the series.''\cite{downey2008novel}

He discusses in some detail the difference between online and offline change detection algorithms, and compares his implementation of a new algorithm with implementations of existing and established algorithms.}

\acite{Killick2011}{
    This paper carries out research similar in some ways to that being conducted in this thesis. Change detection methods are briefly explained and then applied to time series data from an oceanographic study. The ability for change detection approaches to correctly identify the onset and end of `storm seasons' - which form the ground truth in this study.
}

\acite{Matteson2012}{
    A comparison study of several change point detection methods. This study utilises the Rand Index as a measure for comparing algorithm output against a ground truth, and the Adjusted Rand Index for computing the similarity between two computed outputs. The approaches are then applied to various real world data sets from fields such as genetics and finance. This paper formed the basis for the decision to concentrate mainly on the performance of clustering metrics in the field of change point detection evaluation.
}

\acite{Killick2011a}{
    The paper that initially proposed the PELT algorithm for change point detection. The paper mainly concentrates on the computational cost of the PELT algorithm, comparing to other, existing methods of change point computation. Some analysis on the basis of algorithm accuracy is carried out, though limited to the use of binary classification measures.
}

\acite{Pelecanos2010}{
    A paper written concerning possible approaches for the detection of disease outbreaks. This publication was not interesting in terms of the approaches used, but rather in how they were evaluated. This paper restricts itself to the use of binary classification measures, tallying true/false positives and true/false negatives.
}

\acite{Qahtan2015}{
    An evaluation of change point detection methods, using traditional binary classification measures. The measure results are presented purely in the form of true positives, late detections, false positives and false negatives, and thus are easily digested and if necessary translated into the usual terms of recall, precision and F1.
}

\acite{Amigo2009}{
    A detailed evaluation of BCubed as a measure for document clustering. It was based off of this paper that BCubed was chosen as an additional metric to test. BCubed has a good reputation for accuracy, and this paper backs up the ability for BCubed to be effective with detailed comparisons of it's constraints and ability to perform correctly in various situations.

}

\acite{Alvanaki2011}{
    An alternative method of achieving the business goals of the host organisation of this thesis, \citetitle{Alvanaki2011} provides an explanation and example implementation of a method for detecting changes in \emph{conversation topic} as opposed to conversation volume.
}

\acite{Basseville1993}{
    The seminal text on change point detection methods and evaluation. This text is considerably old at the time of writing, but is still an invaluable source of insights and explanations as to how change point detection works, and the criteria by which they should be evaluated.
}

\acite{Dasu2009}{
    An interesting text discussing the application of distributional shift detection as a change detection measure. The approach is not necessary applicable to the work being carried out in this thesis, but it is interesting to read nonetheless - considering the differences between this approach and those tested in this research.
}

\acite{Ginsberg2009}{
    Another study in the same vein as \cite{Pelecanos2010}, this time carried out by Google. This study uses data streams to attempt to predict influenza outbreaks with surprising effectiveness. Here, a log-likelihood approach is used to determine the probability that a random visit by a doctor is related to an illness like influenza.
}

\acite{Kulldorff2005}{
    An additional study in the same category as \cite{Ginsberg2009} and \cite{Pelecanos2010}, it is interesting to this project due to it's use of observed/expected cases to prove the veracity of their prediction model, in the same manner as binary classification scores are utilised in other publications.
}

\acite{Tran2014}{
    A paper discussing the importance of change point detection techniques when applied to \emph{big data} scenarios. No comparison study is carried out, but the paper does serve to espouse the importance of change point detection in scenarios where the sheer \emph{volume} of data may serve to make many approaches impractical.
}

\acite{Xu2011}{
    This study compares various approaches for the application of event detection in data streams. It provides experiments evaluating approaches for topic change detection in documents and news streams, and also event detection in surveillance video streams. In a similar fashion to \cite{Alvanaki2011}, it focuses mainly on using analysis techniques to spot changes in topic in documents as opposed to simply measuring volume. 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background \& Context}

\section{Change Detection Algorithms}

Change detection first came about as a quality control measure in manufacturing, and methods within this domain are generally referred to as \emph{control charts}. Since the inception of approaches such as CUSUM \cite{Page1954} that provide the possibility for on-line evaluation of continuous data streams, change detection has grown as a field. With applications such as epidemic detection, online reputation management and infrastructure error detection, change detection is hugely useful both as an academic problem and in production systems of myriad application.

This thesis work utilises three different change point detection methods, discussed forthwith.

\subsection{Pruned Exact Linear Time}

Pruned Exact Linear Time (henceforth referred to as \emph{PELT}) is a modern change point detection method proposed by \citeauthor{Killick2011a} in \citetitle{Killick2011a}, in 2012 \cite{Killick2011a}.

PELT is an \emph{exact} method of computing change points, based on segmentation of data streams, in the same manner as Segment Neighbourhood and Binary Segmentation. PELT is a modification of the \emph{Optimal Partitioning} method proposed by Brad Jackson et. al in 2003, in \cite{Jackson2003}.

Jackson's method has a computational cost of $O(n^2)$, while Killick's PELT method improves on this with a computational complexity of $O(n)$ \cite{Killick2011a}.

The PELT method works similarly to Segment Neighbourhood \cite{Auger1989} and Binary Segmentation \cite{Jackson2003} \cite{Yao1984} approaches, in that it attempts to fit a model of segmentation of a given data set, minimising the cost of the segmentation to achieve an optimal distribution of segments and therefore change points.

Algorithm \autoref{alg:pelt} describes a pseudocode implementation of the PELT algorithm, as described in \cite{Eckley2011}\newline

\begin{algorithm}[H]
    \label{alg:pelt}
    \caption{PELT Method for change point detection}
    \DontPrintSemicolon
    \SetKwInOut{Initialise}{Initialise}
    \KwIn{A set of data of the form, $(y_1, y_2, \ldots,y_n)$ where $y_i \in \mathbb{R}$.\newline
        A measure of fit $C(.)$ dependent on the data.\newline
        A penalty constant $\beta$ which does not depend on the number or location of change points.\newline
        A constant $K$ that satisfies $C(y_{(t+1):s}) + C(y_{(s+1):T}) + K \leq C(y_{t+1):T})$\newline}
    \Initialise{Let $n$ = length of data and set $F(0) = -\beta$, $cp(0) = NULL$\newline}
    \For{$\tau* = 1,\ldots,n$}{
        Calculate $F(\tau*) = min_{\tau \in R_{\tau*}} \lbrack F(\tau) + C(y_{\tau+1:\tau*}) + \beta \rbrack$\;
        Let $\tau^1 = arg \Big\{ min_{\tau \in R_{\tau*}} \lbrack F(\tau) + C(y_{(\tau+1):\tau*}) + \beta \rbrack \Big\}$\;
        Set $cp(\tau*) = \lbrack cp(\tau^1), \tau^1 \rbrack$\;
        Set $R_{\tau^* + 1} = \{\tau \in R_{\tau^*} \cup \{\tau^*\} : F(\tau) + C(y_{\tau+1:\tau^*}) + K \leq F(\tau^*)\}$
    }
    \KwOut{the change points recorded in $cp(n)$}
\end{algorithm}

\subsection{Segment Neighbourhoods}

Proposed by Auger and Lawrence in 1989 \cite{Auger1989}, Segment Neighbourhood (\emph{SegNeigh}) is another example of an exact segmentation algorithm for the detection of change points in data. SegNeigh utilises dynamic programming to search the segmentation space (defined as the maximum number of change points in a given data stream) and then compute the cost function for every possible segmentation of the data. In this way, the location and number of change points can be computed exactly by taking the segmentation that returns the lowest cost function result.

Segment Neighbourhoods has a computational complexity of $O(n^2)$, significantly higher than that of PELT or Binary Segmentation. However, this additional cost in complexity is offset by improved performance, as shown by \citeauthor{Braun2000} in \citetitle{Braun2000} \cite{Braun2000}.

Algorithm \autoref{alg:segneigh} describes a pseudocode implementation of the SegNeigh algorithm, as described in \cite{Eckley2011}\newline

\begin{algorithm}[H]
\label{alg:segneigh}
    \caption{Generic Segment Neighbourhoods method for change point detection}
    \DontPrintSemicolon
    \SetKwInOut{Initialise}{Initialise}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \KwIn{A set of data of the form, $(y_1, y_2, \dots, y_n)$\newline
        A measure of fit $R(.)$ dependent on the data which needs to be minimised.\newline
        An integer, $M-1$ specifying the maximum number of change points to find.}
    \Initialise{Let $n$ = length of data.\newline
        Calculate $q^1_{i,j} = R(y_{i:j})$ for all $i, j \in \lbrack 1,n \rbrack$ such that $i < j$.\newline}
        
    \For{$m = 2, \ldots ,M$}{
        \For{$j \in \{1,2,\ldots , n\}$}{
            Calculate $q^m_{1,j} = min_{v \in \lbrack 1,j \rbrack }(q^{m-1}_{1,v} + q^1_{v + 1, j})$\;
        }
        Set $\tau_{m, 1}$ to be the $v$ that minimises $(q^{m-1}_{1,v} + q^1_{v + 1, n})$\;
        \For{$i \in \{ 2, 3, \ldots , M \}$}{
            Let $\tau_{m,i}$ to be the $v$ that minimises $(q^{m-i-1}_{1,v} + q^1_{v + 1, cp_{m,i-1}})$\;
        }
        
    }
        
    \KwOut{For $m = 1, \ldots , M$: the total measure of fit, $q^m_{1,n}$ for $m-1$ change points and the location of the change points for that fit, $\tau_{m,1:m}$.}
\end{algorithm}

\subsection{Binary Segmentation}

Binary Segmentation \cite{Jackson2003} \cite{Yao1984} is a popular method for change point detection, widely utilised in the field. Binary segmentation is a method that recursively applies a single change point detection method. On the first iteration, if a change point is detected, the data is split around that change point (resulting in two data sets) and the change point method is run again on the two resulting data sets. This process is repeated such that many data set segments are created, and runs until no further change points are detected.

Binary Segmentation is an \emph{approximate} change point detection approach, and returns estimated change point locations - unlike PELT and SegNeigh - which return exact change point locations. It has the same computational cost as PELT, $O(n)$.

Algorithm \autoref{alg:binseg} describes a pseudocode implementation of the BinSeg algorithm, as described in \cite{Eckley2011}\newline

\begin{algorithm}[H]
    \label{alg:binseg}
    \caption{Generic Binary Segmentation method for change point detection}
    \DontPrintSemicolon
    \SetKwInOut{Initialise}{Initialise}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \KwIn{A set of data of the form, $(y_1, y_2, \dots, y_n)$\newline
    A test statistic $\Lambda(.)$ dependent on the data\newline
    An estimator of change point position $\hat{\tau}(.)$\newline
    A rejection threshold $C$.\newline}
    \Initialise{Let $C=\emptyset$, and $S = \{ \lbrack 1,n \rbrack \}$\newline}
    \While{$S \neq \emptyset$}{
        Choose an element of $S$, denote element as $\lbrack s,t \rbrack$\;
        \If{$\Lambda(y_{s:t}) < C$}{
            remove $\lbrack s,t \rbrack$ from $S$\;
        }
        \If{$\Lambda(y_{s:t})$}{
        
            remove $\lbrack s, t \rbrack$ from $S$\;
            calculate $r = \hat{\tau}(y_{s:t}) + s - 1$, and add $r$ to $C$\;
            
            \If{$r \neq s$}{
                add $\lbrack s, r \rbrack$ to $S$\;
            }
            
            \If{$r \neq t - 1$}{
                add $\lbrack r + 1, t \rbrack$ to $S$\;
            }
        }
    }
    \KwOut{the set of change points recorded in $C$}
\end{algorithm}

\section{Algorithm Configuration}

Change detection is an \emph{unbounded} problem. Left without some system of constraint, the algorithm could theoretically run to infinity. Indeed, one of the algorithms utilised in this research, PELT\todo{citation needed}, when left unbounded, will detect every data point in the time-series as a change point. This result is \textit{technically} correct, but not useful for our purposes. For this reason, the algorithms implement a penalty system, allowing for an optimal number of change points to be detected.

\subsection{Penalty Scores}

Penalty scores operate as a mechanism for optimising an unbounded problem such as the one being addressed here. Haynes et al. define the problem as follows \cite{Haynes2014}: Given time series data points $y_1,\ldots,y_n$, the series will contain $m$ change points such that their locations $\tau_{1:m} = (\tau_1,\ldots,\tau_m)$, where $\{\tau_i \in \mathbb{Z} \mid 1 \leqslant \tau_i \leqslant n-1\}$. $\tau_0$ is assumed to be 0, and $\tau_{m+1}$ is assumed to be $n$. In this way we can state that a given change detection algorithm will split the time series into $m + 1$ segments such that segment $i$ contains the points $y_{(\tau_{i-1}+1):\tau_i} = (y_{\tau_{i-1} + 1},\dots,y_{\tau_i})$. A \emph{cost function} for a segmentation of the data set $y_s$ can be defined as $C = (y_{s+1:t})$. This cost function defines a cost for a given segmentation containing points $y_{s+1:t}$. To illustrate, the cost function $C$, in the context of binary classification, returns a value that increases for an `incorrect' classification and decreases for a `correct' classification.

At this point it can be stated that the \emph{constrained minimisation problem} is as follows:

\begin{equation}
\label{unbounded}
    Q_m(y_{1:n}) = \min_{\tau_{1:m}} \Bigg\{ \sum^{m+1}_{i=1}[C(y_{(\tau_{i - 1} + 1):\tau_i})] \Bigg\}
\end{equation}

\autoref{unbounded} is an equation showing the optimisation problem that change point detection presents. If the equation is taken intuitively, it shows that for a dataset with a \emph{known} number of change points, the segmentation of the data around the change points can be effectively estimated by obtaining the segmentation that returns the minimum cost based on the cost function $C$.

However, the problem being solved by many change detection algorithms involves an unknown number of change points, at unknown locations. In this case we can estimate the following to obtain the number and location of the change points:

\begin{equation}
\label{eqn:penalty}
    \min_{m} \big\{ Q_m(y_{1:n}) + f(m) \big\}
\end{equation}

\autoref{eqn:penalty} includes a \emph{penalty function} $f(m)$ that increases proportionally with the number of change points $m$. Essentially, as the segmentation calculated as $Q_m$ increases in size, so does the result of the penalty function $f(m)$. These two elements are at-odds with each other, presenting a problem that requires optimisation to correctly estimate the number and location of change points in the data set $y_s$. If $f(m)$ increases in a linear fashion with $m$, we can define a \emph{penalised minimisation problem} in the following manner:

\begin{equation}
\label{bounded}
    Q_m(y_{1:n}, \beta) = \min_{m,\tau_{1:m}} \Bigg\{ \sum^{m+1}_{i=1}[C(y_{(\tau_{i - 1} + 1):\tau_i}) + \beta] \Bigg\}
\end{equation}

In this equation, $\beta$ is the penalty function defined previously as $f(m)$.

There are a number of established approaches to calculating penalty values for unbounded problems, chiefly among which are Schwarz Information Criterion (SIC)/Bayesian Information Criterion (BIC) \cite{Schwarz1978}, Akaike Information Criterion (AIC) \cite{Akaike1974} and Hannan-Quinn \cite{Journal2009}. Of these approaches, it is necessary to experiment to find the scheme that produces the `correct' number of change points for a given dataset. The penalty schemes are defined as follows:

\begin{align}
    \text{SIC} &= \ln(n) k - 2 \ln(\hat{L})\\
    \text{AIC} &= 2k - 2 \ln(\hat{L})\\
    \text{HQC} &= -2L_{max} + 2k \ln(\ln(n))
\end{align}

Where $n$ is the total number of observations, $k$ is the number of \emph{free parameters}\footnote{variables that are unable to be predicted by a given model} and $\hat{L}$ is the maximum value of a likelihood function.\todo{reword, wikipedia}

The penalty function returns a value which is represented by $\beta$ in \autoref{bounded}, which then limits the number of possible segmentations returned by $Q$. In this way, varying the penalty function $\beta$ can alter the results given by a change detection algorithm by increasing or decreasing the number of change points detected.

The \texttt{changepoint} package requires the provision of a penalty function for each algorithm, which has been chosen as `SIC' for this research. This decision is based upon advice published in \cite{Eckley2011}, in which it is stated that AIC (while popular in the field of change point detection as a penalty term) tends to asymptotically overestimate $k$.

While the selection of an optimal penalty term for a given data set and change point detection algorithm is an open research question in itself. There exists a method, \emph{Changepoints for a Range of Penalties} (CROPS) \cite{Haynes2014}, in which a change detection algorithm (PELT, in this case) is repeatedly executed in such a fashion that allows for the plotting of the number change points detected against the penalty value used. When conducting analysis against a single, static data source (offline analysis), this method can be useful for ensuring optimal penalty term selection.

\begin{figure}[h]
\centering
    \includegraphics[width=0.5\textwidth]{figures/CROPS2}
    \caption{CROPS Algorithm Output}
    \label{fig:CROPS}
\end{figure}

\autoref{fig:CROPS} shows an example of the output of the CROPS algorithm, when executed against the `Rabobank' data set used in the real world data analysis. To read this output, the optimal penalty value should be chosen such that the plot at that point has begun to level off, showing a fast increase in the number of change points for penalty values lower than this. This example shows an optimal penalty value of about 6 (backed up by the algorithm output of a penalty value of 6.21 for 2 change points - signified by the dash-dot red line on the plot). The penalty value for the same data set computed with SIC is 8.16 (signified by the dotted red line on the plot), which also results in a total of two change points being detected. Because the PELT algorithm has a computational complexity of $O(n)$, if it is necessary to carry out an analysis such as this for a static data set, it would be easy to do so.

However, the usefulness of such a method for application to streaming (online) data sets is questionable.


\subsection{Distribution Assumptions}

It is also important to note that many change point detection methods make an assumption about the underlying distribution of the data being analysed. This distribution can be, for example, a normal distribution, poisson distribution or a gamma distribution. The selection of this distribution can have an effect on the ability of a given algorithm to detect change points.

To limit the scope of this research, an assumption is made that all of the data being analysed follows a normal distribution, to allow for a `like for like' comparison of results. It is possible for some algorithms to use a CUSUM test statistic that makes no assumptions about data distribution, but this is not supported by all of the methods implemented in \texttt{changepoint}. It is also possible to specify a poisson or gamma distribution when utilising the mean/variance test statistic, but this has not been done in this research, again, in order to ensure a `like for like' comparison of approaches.

\section{Evaluation Measures}

As briefly discussed in the introduction to this thesis, there are a number of pre-existing approaches for the evaluation of change detection methods. Here, the measures being evaluated are briefly explained:

\subsection{F1 Score}

This measure is utilised for testing accuracy in problems of binary classification. It considers two different measures, \emph{precision} and \emph{recall}, and takes a harmonic mean of the two measures to compute the final score. This measure is used in \cite{Qahtan2015}, \cite{Buntain2014}, and \cite{Pelecanos2010} (among others) for the purposes of evaluating change point detection methods.
	
	To calculate Precision and Recall, a \emph{confusion matrix} such as that in \autoref{confusionmatrix} is first constructed to provide values for the total number of true positives, false negatives, false positives and true negatives:
	
	\begin{figure}[H]
	\begin{center}
	    \begin{tikzpicture}[box/.style={draw,rectangle,minimum size=2cm,text width=1.5cm,align=center}]
            \matrix (conmat) [row sep=.1cm,column sep=.1cm] {
            \node (tpos) [box,
               label=left:\( \mathbf{p'} \),
                label=above:\( \mathbf{p} \),
            ] {True \\ Positive};
            &
            \node (fneg) [box,
                label=above:\textbf{n},
                label=above right:\textbf{Totals},
                label=right:\( \mathrm{P}' \)] {False \\ Negative};
            \\
            \node (fpos) [box,
                label=left:\( \mathbf{n'} \),
                label=below left:\textbf{Totals},
                label=below:P] {False \\ Positive};
            &
            \node (tneg) [box,
                label=right:\( \mathrm{N}' \),
                label=below:N] {True \\ Negative};
            \\
            };
    
            \node [rotate=90,left=.05cm of conmat,anchor=center,text width=1.5cm,align=center] {
                \textbf{Actual Value}
            };
            \node [above=.05cm of conmat] {\textbf{Prediction Outcome}};
            
        \end{tikzpicture}
        \caption{Confusion Matrix Example}
        \label{confusionmatrix}
        \end{center}
	\end{figure}
		
	Precision and Recall were proposed by \citeauthor{Kent1955}, in \citetitle{Kent1955}, for the purposes of scoring the effectiveness of information retrieval methods.
		
	$Recall$ is computed as the number of correct positive results, divided by the number of positive results that should have been detected \cite{Kent1955}. $Precision$ is computed as the number of correct positive results divided by the number of all possible positive results \cite{Kent1955}:
	
	\begin{eqnarray}
	    Precision = \frac{TP}{TP + FP}\\
	    Recall = \frac{TP}{TP + FN}
	\end{eqnarray}
		
	The F1 score calculation (the harmonic mean of both recall and precision) can be described in general terms as follows \cite{VanRijsbergen1979}:

	\begin{equation}
		F_1 = 2 \cdot \frac{1}{\frac{1}{recall} + \frac{1}{precision}} = 2 \cdot \frac{precision \cdot recall}{precision+recall}
		\label{equ:F1}
	\end{equation}

    The F1 score was first proposed by \citeauthor{VanRijsbergen1979} in his seminal work, \citetitle{VanRijsbergen1979}.

	As the F1 score is a binary classification measure, it can only be used to test the precision of an algorithm in a single domain, that is, was the change detected or not. Precision, Recall and F1 Score all provide a score $s$ such that $ \{s\in\mathbb{R} \mid 0\leq s \leq 1\} $.

\subsection{Rand Index}

This measure is for computing the similarity between two clusters of data points. It is used for calculating the overall accuracy of a given clustering approach, when compared against a set of ground truth clusters. It was first proposed by \citeauthor{Rand1971} in \citetitle{Rand1971}. It was used in \cite{Matteson2012} for the purposes of evaluating change point detection approaches. The Rand Index is defined as \cite{Rand1971}:

	\begin{equation}
		R = \frac{a+b}{a+b+c+d}
	\end{equation}
	
	Given a set of data points $S$, partitioned through two different methods, which we shall refer to as $X$ and $Y$. Based on this knowledge the following can be defined:
	
	\begin{itemize}
		\item $a$ = total number of pairs that were partitioned into the \emph{same} subset by both $X$ and $Y$
		\item $b$ = total number of pairs that were partitioned into \emph{different} subsets by both $X$ and $Y$
		\item $c$ = total number of pairs that were partitioned into the \emph{same} subset by $X$ and into a different subset by $Y$
		\item $d$ = total number of pairs that were partitioned into the \emph{same} subset by $Y$ and into a different subset by $X$
	\end{itemize}\todo{reword, wikipedia}
	
	Intuitively, it can be stated that $a+b$ is the total number of agreements between methods $X$ and $Y$, while $c+d$ is the total number of disagreements. This calculation will return $0$ for completely different clusters and $1$ for identical clusters. The Rand Index provides a score $s$ such that $ \{s\in\mathbb{R} \mid 0\leq s \leq 1\} $
	
\subsection{Adjusted Rand Index}
	
Similar to the Rand Index, but adjusted to take into account the random chance of pairs being assigned to the same cluster by both approaches being compared. While the Rand Index is suited for comparing a segmentation method against a known-good \emph{oracle} method, the adjusted index is more suited to comparing two differing approaches \cite{Matteson2012}. It was first proposed by \citeauthor{Hubert1985} in \citetitle{Hubert1985}. It was utilised as a similarity measure for computed clusters created by segmentation of data sets in \cite{Matteson2012}. It is defined as \cite{Hubert1985}:
	
	\begin{equation}
	    R_{adjusted} = \frac{R - R_{expected}}{R_{max} - R_{expected}}
	\end{equation}
	
	where $R_{expected}$ is defined as the expected Rand Index score for two completely random classifications of the given data \cite{Matteson2012} and $R_{max}$ is defined as the maximum Rand Index value - generally $1$.
	
	The Adjusted Rand Index provides a score $s$ such that $ \{s\in\mathbb{R} \mid -1\leq s \leq 1\} $. This is different to the score returned by the `basic' Rand Index, in that it is possible for a negative value to be returned.
		
\subsection{BCubed}
		
BCubed is similar to the Adjusted Rand Index, in that it is adjusted to be appropriate for all constraints in a clustering problem. It was developed by Bagga and Baldwin in 1998 \cite{Bagga1998}.

This measure was not found to be in use for the purposes of evaluating change point detection methods, but it was particularly interesting to investigate based on \citeauthor{Matteson2012}'s use of clustering metrics in \cite{Matteson2012}. It was decided that this measure would be included as an additional well-known clustering measure, to investigate whether it would perform any better than the established Rand and Adjusted Rand indices.
	
	BCubed operates in a similar way to the Recall and Precision classification metrics, in that it computes precision and recall values, before calculating a harmonic mean of the two metrics. BCubed also makes use of an additional \emph{correctness} function defined as follows, where $e$ and $e'$ are equivalent elements in a ground truth and computed clustering respectively, $L$ is a computed label and $C$ is a ground truth category:
	
	\begin{equation}
	\label{eqn:correctness}
	    \text{Correctness}(e,e') = 
	    \begin{cases}
	        1 & \text{iff $L(e) = L(e') \leftrightarrow C(e) = C(e')$}\\
	        0 & \text{otherwise}
	    \end{cases}
	\end{equation}
	
	\autoref{eqn:correctness} can be stated intuitively as returning $1$ if and only if both the ground truth and computed clusterings place the element $e$ into the same cluster, returning $0$ in all other cases.
	
	Precision and Recall can then be calculated as follows:
	
    \begin{align}
    \label{eqn:bcubed-precision}
        \text{Precision} &= Avg_e [Avg_{e' . C(e) = C(e')}[Correctness(e,e')]]\\
        \label{eqn:bcubed-recall}
        \text{Recall} &= Avg_e [Avg_{e' . L(e) = L(e')}[Correctness(e,e')]]
    \end{align}

To obtain the F score, a harmonic mean of Precision and Recall is taken in the same manner as the F1 score for binary classification:

\begin{equation}
    F_{bc} = 2 \cdot \frac{1}{\frac{1}{Recall_{bc}} + \frac{1}{Precision_{bc}}}
\end{equation}

The BCubed F-Score provides a score $s$ such that $ \{s\in\mathbb{R} \mid 0\leq s \leq 1\} $, much like the Rand Index and F1 score for binary classification.

\chapter{Research Method}
\label{Research Method}

\section{Introduction}

The purpose of this thesis is twofold: firstly, to conduct a meta analysis of evaluation measures, and how they perform in the domain of change point detection problems. Being that these measures are metrics designed for other problems that are not necessarily change point detection, it stands to reason that there are perhaps situations where families of metrics will disagree with each-other, or disagree with by-eye evaluation by domain experts. The second purpose is to evaluate the veracity of a number of existing change point detection algorithms when they are applied to data from social media.

This thesis consists of two distinct parts:

\begin{itemize}
  \item Meta-analysis of evaluation measures, fulfilling RQ2 and associated sub-questions.
  \item Application of change detection approaches to real world data and evaluation based on requirements elicitation and evaluation measures.
\end{itemize}

\section{Evaluation Pipeline Construction}

This thesis will analyse three different change detection algorithms: \emph{Pruned Exact Linear Time}\cite{Killick2011a}, \emph{Binary Segmentation} \cite{Jackson2003} and \emph{Segment Neighbourhoods}\cite{Auger1989}. These will be referred to as \emph{PELT}, \emph{BinSeg} and \emph{SegNeigh} respectively. The algorithms will be briefly discussed in this thesis, along with the chosen critical values such as \emph{minimum segment length}, \emph{penalty scoring} and \emph{assumed underlying distribution}.

The algorithms will then be applied to a collection of datasets falling into two categories: real-world conversation volume data taken from Twitter, and simulated data generated according to certain constraints which will also be discussed here.

The results provided by each technique will then be evaluated according to the following measures: Precision, Recall, F1 score, Rand Index, Adjusted Rand Index, Bcubed Precision, Bcubed Recall and Bcubed F-Score. A meta-analysis will take place to evaluate the effectiveness of these methods when compared with each other and by-eye analysis from social media domain experts.\todo{maybe not}

The method of evaluating the approaches will be developed using a combination of Python and \textsf{R}. \textsf{R} is a combined language and environment created for the purpose of statistical computing \cite{RCoreTeam2017}.

Python is being utilised also due to the availability of relevant software packages for the purposes of evaluation measure calculation.

\subsection{Calculation of Changepoint Locations}

\texttt{changepoint} is a powerful R package that provides a number of different change detection algorithms, along with various approaches to penalty values. \texttt{changepoint} offers change detection in mean, variance and combinations of the two, using the AMOC, PELT, Binary Segmentation and Segment Neighbourhood algorithms\todo{cite these}.

\texttt{Changepoint} was developed by Rebecca Killick and Idris A. Eckley and is provided free of charge under the GNU general public license \cite{Killick2014}.

\subsection{Calculation of Evaluation Measures}

For the calculation of Precision, Recall and F1 Score, the \textsf{R} package \texttt{caret} is being utilised. \texttt{caret} is an acronym for `Classification and Regression Training, and provides a number of tools for data manipulation, model creation and tuning, and performance measurements. Through the use of this package, it is possible to generate a \emph{confusion matrix} based on algorithm output, and generate various performance measures based upon this. \texttt{caret} was written by Max Kuhn, and is provided free of charge under the GPL license \cite{FromJedWing2017}.

For the calculation of the Rand Index and Adjusted Rand Index the \textsf{R} package \texttt{phyclust} is being used. This package was developed by Wei-Chen Chen, for the purposes of providing a phyloclustering implementation. While this approach is not something being examined in this thesis, the package does provide an implementation of the Rand Index and Adjusted Rand Index metrics, which are relevant to this research. This package is also provided free of charge under the GPL license \cite{Chen2011}.

Calculating the BCubed precision, recall and f-score metrics is being carried out in Python, using the \texttt{python-bcubed} project. This is a small utility library for the calculation of BCubed metrics, developed by Hugo Hromic and provided under the MIT license \cite{Hromic2016}. In order to interface with this library via \textsf{R}, the package \texttt{rPython} is being used. This package provides functionality for running Python code and retrieving results in \textsf{R}, and was developed by Carlos J. Gil Bellosta. It is provided under a GPL-2 license \cite{Bellosta2015}.

Additionally, the \texttt{ROCR} package was utilised during the research phase of this project to generate Receiver Operating Characteristic (ROC) curves - a method used by some publications to evaluate change point detection methods. While ROC curves are not being utilised to produce the results published in this thesis, it was nonetheless an important part of the research, and provided useful insights into binary classification as a field and as a method of evaluation. \texttt{ROCR} was developed by Tobias Sing et. al. and is provided free of charge under a GPL license \cite{Sing2005}

\section{Measures Meta-Analysis}
\label{meta analysis explainer}

The experiments hereunder were developed to fulfil the requirement to answer research question 2, and it's associated sub-questions:

\begin{itemize}
    \item Are measures not specifically defined for the purpose of change detection evaluation effective in this domain?
    \begin{itemize}
        \item In what way do established measures fail to behave correctly?
        \item Which evaluation measure provides the best evaluation, given the functional requirements set forth by the host organisation?
    \end{itemize}
\end{itemize}

The research questions above are to be answered using a series of experiments carried out using simulated data and algorithms. The experiments intend to test the following criteria:

\begin{enumerate}
    \item Dependence on sample size (otherwise referred to here as time series length)
    \item Impact of data preceding a known change point and its detection
    \item Impact of data following a known change point and its detection
    \item Ability to provide a temporal penalty for late, early or exact detections
    \item Ability to penalise correctly for false positives
    \item Ability to penalise correctly for false negatives
    \item Impact of change point density in an analysed time series
\end{enumerate}

What follows is a listing of the experiments carried out. Illustrative pseudocode algorithms for each experiment can be found in appendix A of this document.

\subsection{Experiment Listings}

For each experiment description, the sample size (otherwise known as time series length) is denoted by $n$, the true change point(s) in the time series is denoted as $\tau_n$ and any `detected' change points are denoted by $\tau'_{n}$. For each experiment, the changing variable (the iterator in the \textsf{R} source code) is denoted as $i$. For experiments where the computed change point(s) $\tau'_n$ are placed with some error, this error is denoted as $\Delta \tau$. $\Delta \tau$ can always be expected to adhere to $\{ \Delta \tau \in \mathbb{R} \mid \Delta \tau \geq 0 \}$, as values $< 0$ can be considered successful, early detections - and thus do not constitute an error.

\subsubsection{Experiment 1}

This experiment involves increasing the `head' (sample size prior to a single known change point) of a time series, prepending to and thus lengthening the time series. The time series contains a single known change point, and a detected change point provided by a pseudo-algorithm is placed such that it is placed with a $\Delta \tau$ of 5.

For each prepended point, all of the evaluation metrics are calculated and plotted. The experiment evaluates both criteria 1 and 2.

The experiment runs such that $n = 55$ on commencement, and $n = 500$ upon completion, with each iteration adding a single data point such that $i = 445$ on completion. For each iteration, $\tau = i + 1$ and $\tau' = \tau + 5$.

For this experiment, the null hypothesis $H_0$ is that neither additional points before a change point, nor data set length, will have an affect on the scores provided by evaluation metrics.

The alternative hypothesis $H_1$ is that the measures will be affected in some way by additional points before the change point. It is also hypothesised that data set length will have an affect on the metric value.


\subsubsection{Experiment 2}

This experiment involves increasing the `tail' (sample size after a single known change point) of a time series, appending to and thus lengthening the time series. The time series contains a single known change point $\tau$ and a detected change point $\tau'$ provided by a pseudo-algorithm with a $\Delta \tau$ of 5.

For each appended point, all of the evaluation metrics are calculated and plotted. The experiment evaluates both criteria 1 and 3.

The experiment runs such that $n=55$ on commencement, and $n=550$ upon completion, with each iteration adding a single data point such that $i$ runs from 2 to 500. For each iteration, $\tau = 51$ and $\tau' = 56$.

For this experiment, the null hypothesis $H_0$ is that neither additional points after a change point, nor data set length, will have an affect on the scores provided by evaluation metrics.

The alternative hypothesis $H_1$ is that the measures will be affected in some way by additional points after the change point. It is also hypothesised that data set length will have an affect on the metric value.

\subsubsection{Experiment 3}

This experiment involves moving a known change point and a computed change point through a fixed length time series. The time series contains a single known change point $\tau$ and a detected change point $\tau'$ with $\Delta \tau = 5$, provided by a pseudo-algorithm.

For each iteration of the experiment, all of the evaluation metrics are calculated and plotted. The experiment evaluates both criteria 1 and 2.

The experiment runs such that $n$ is constant at $n=501$. For each iteration, $\tau = i$ and $\tau' = i + 5$, for values of $i$ from 5 to 500.

For this experiment, the null hypothesis $H_0$ is that neither additional points before or after a change point, nor data set length, will have an affect on the scores provided by evaluation metrics.

The alternative hypothesis $H_1$ is that the measures will be affected in some way by additional points before or after the change point. It is also hypothesised that data set length will have an affect on the metric value.

\subsubsection{Experiment 4}

This experiment involves moving a detected change point $\tau'$ provided by a `pseudo-algorithm' through a time series of fixed length, thus evaluating the ability of a measure to provide a temporal penalty for late, early, or exact detections.

For each iteration of the experiment, all of the evaluation metrics are calculated and plotted. The experiment evaluates criterion 4.

The experiment runs such that $n$ is constant at $n=500$. For each iteration $\tau = 51$ and $\tau' = i$, where $i = 51$ at the commencement (such that $\tau = \tau'$) and $i = 500$ at completion. Thus, values of $\Delta \tau$ range from 50 to 500.

For this experiment, the null hypothesis $H_0$ is that all of the calculated metrics will correctly issue a penalty for late detections of a change point, in all instances.

The alternative hypothesis $H_1$ is that one or more measures will incorrectly issue scores for a detected change point occurring various distances from the true change point.

\subsubsection{Experiment 5}

This experiment involves adding `false positive' change point detections of various values of $\Delta \tau$ to a time series of fixed length with a single known change point $\tau$.

For each iteration of the experiment, all of the evaluation metrics are calculated and plotted. The Experiment evaluates criterion 4.

The experiment runs such that $n$ is constant at $n=500$. For each iteration $\tau = 51$ and $\tau' = i$, where $i = 51$ at the commencement of the experiment, $i=55$ at the second iteration, and increases in increments of 5 until $i = 500$. This serves to move the detected change point through the time series, starting at the true change point and progressing to the end of the time series, ending at the final point of the time series.

For this experiment, the null hypothesis $H_0$ is that all of the metrics will correctly penalise for false positive detections in a time series.

The alternative hypothesis $H_1$ is that the measures will not correctly issue penalties for false positive detections.

\subsubsection{Experiment 6}

Removing `false negative' results (by way of explanation, adding correct detections) to a fixed length time series, thus increasing the number of correctly detected change points on each iteration.

The experiment begins with a time series of $n=900$ with $\tau$ being situated at intervals of 100. As the experiment progresses, $\tau'$ points are added such that $\tau_n = \tau'_n$ for each iteration. Thus, adding a computed change point detection at the same place as a ground truth detection (a $\Delta \tau$ of 0)

This experiment evaluates criterion 6, by recalculating the scoring metrics upon each iteration.

For this experiment, the null hypothesis $H_0$ is that removing false negative results will result in a linear increase in score from each metric.

The alternative hypothesis $H_1$ is that the measures will not appear to correctly increase in value as false negatives are removed from the data set - showing instead inconsistent behaviour.

\subsubsection{Experiment 7}

Adding both true change points and detected change points from a `pseudo algorithm' that provides detections every 25 points to a time series, by appending to it, and thus increasing it's length. The experiment runs from $n=50$ until $n=1000$ in increments of 50. Each $\tau$ and $\tau'$ is placed upon a new iteration such that each $\tau'_n = \tau_n + 2$, giving a $\Delta \tau$ value of 2. This experiment tests both criteria 1 and 7.

For this experiment, the null hypothesis $H_0$ is that metrics will be unaffected by change point density in a time series, nor will they be affected for changes in time series length.

The alternative hypothesis $H_1$ is that the measures will be affected in some way by change point density and data set length, showing an increase or decrease in score value as density and length increases.

\subsubsection{Experiment 8}

Adding both true change points and detected change points from a `pseudo algorithm', to a time series of fixed length. The experiment runs with a static data set size of $n=1050$, and adds two $\tau$ and $\tau'$ on each iteration, effectively `lifting' a spike out of a static data set where all values are 0 at the start of the experiment. Each $\tau'$ is placed such that $\tau'_n = \tau_n + 2$ (a $\Delta \tau$ of 2, simulating a slightly late detection. This ensures that the measures do not report perfect detections throughout the experiment, thus making it impossible to spot how change point density in a fixed length data set affects the calculation of metrics.

This experiment fulfils criteria 7.

For this experiment, the null hypothesis $H_0$ is that metrics will be unaffected by change point density, maintaining a constant value.

The alternative hypothesis $H_1$ is that the measures will be affected in some way by changes in change point density. The hypothesis is that the value of the measures will increase or decrease as change point density increases.

\section{Comparison of Measures Based Upon Functional Requirements}

As part of the research being carried out, a set of functional requirements are elicited from the host organisation. Based on these functional requirements the measures are evaluated and compared in such a manner that we can choose the `best' measure for the use case of the host organisation. The previous research section (\autoref{meta analysis explainer}) provides the results necessary to compare the behaviour of certain metrics in certain situations, against the priorities of the host organisation.

For example, if the host organisation expresses that they require an approach with an absolute minimum of false positives, it is important to judge the application of these algorithms against real world data, utilising measures that correctly penalise for false positive detections in a data stream.

\section{Application of Algorithms to Real World Data}
\label{real world explainer}

\subsection{Data Preparation}

The experiment is being carried out using conversation volume data taken from Buzzcapture's Brand Monitor application. Brand Monitor harvests tweets via the Twitter API and makes them searchable through a bespoke interface as well as providing a number of useful metrics for reputation management. In order to gain data for this experiment, Social Media Analysts from Buzzcapture were requested to provide query strings for the Brand Monitor application, showing a situation where they feel a client should have been informed of a distinct change in conversation volume. The analysts were then asked to manually annotate the data with the points at which they believe a change point detection algorithm should detect a change. In this way, bias is avoided when establishing the ground truth for the data - the author was not involved in this annotation process other than providing instructions. The result of this exercise is then a set of time series' showing the twitter conversation volume over time for a given brand. This is accompanied by a separate data set containing indices at which changes \emph{should} be detected - thus serving as the ground truth for this part of the experiment.

Once the query strings provided were executed, the corpus of test data included the following data sets:

\begin{itemize}
    \item Dirk
    \item Bol.com
    \item Connexion
    \item DAP
    \item Jumbo
    \item Kamer van Koophandel
    \item Rabobank
    \item Tele2
    \item UWV
    \item Ziggo
\end{itemize}

All data sets were trimmed such that the sample size equals 60 for every set. The sets were exported from the BrandMonitor application in CSV format, with daily conversation volume totals.

\subsection{Execution}

To perform the experiment, an \textsf{R} script is executed that reads the CSV file containing the data points being analysed, and reads them into a data frame object. At this point, change point analysis is conducted using the following algorithms and test statistics:

\begin{itemize}
    \item Mean, using PELT
    \item Mean, using SegNeigh
    \item Mean, using BinSeg
    \item Variance, using PELT
    \item Variance, using SegNeigh
    \item Variance, using BinSeg
    \item Mean \& Variance, using PELT
    \item Mean \& Variance, using SegNeigh
    \item Mean \& Variance using BinSeg
\end{itemize}

Once all of the analysis methods have been executed successfully, the change points are extracted and used to compute the various scoring metrics, when the change points are compared against the established ground truth detections.

There are some assumptions made when handling this data. Firstly, for the binary classification measure, as the data being used in this study consists of daily conversation volume statistics, anything other than an \emph{exact} detection when compared with the ground truth is considered a \emph{failure}. Detections of a change a day after the true change point are too late for useful notifications to be sent in a production system. Detections prior to the true change point, while possibly useful in some way for predicting a future change (certainly in a production system), are also considered a \emph{failure}. The clustering measures should not be affected by this assumption, as by their nature they should provide a more granular score for detections slightly before or after the ground truth change point.

Secondly, as discussed before, the algorithms being evaluated require some assumption to be made as to the probabilistic distribution of the data being analysed. Diagnostic histogram plots created prior to experimentation showed that the various data sets varied considerably in terms of the probability distribution of data points. It is possible for algorithms configured to use a mean test to be configured to use a CUSUM test statistic that makes no assumption about the data distribution being analysed. Unfortunately, at the time of the experiments, the CUSUM test statistic was not supported for variance or mean/variance tests. The variance and mean/variance tests allow for the selection of other distributions such as poisson and gamma. As such, all algorithm runs were configured to assume a \emph{normal} probability distribution. While this assumption may not hold for all of the data sets, it is necessary to take this step to ensure a like for like comparison as much as possible.

After all of the metrics are calculated and tabulated, it is possible to see which algorithm performed the `best', and also carry out a comparison analysis to see which algorithms provided the most consistent results against all data sets.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Research}

\section{Introduction}

This section of the thesis describes the execution, results and conclusions of each individual study carried out. It is divided into three sections:

\begin{itemize}
    \item Simulation studies - taking simulated data and simulated algorithm results to analyse the impact of various cases on calculated measures.
    \item Analysis of measures - taking the results of the simulation studies and testing their ability to match the functional requirements of the host organisation.
    \item Real-World Data - taking a set of real world twitter data and analysing the ability of change detection algorithms to detect changes in the data compared with established ground truth change points.
\end{itemize}

\section{Simulation Studies}

The simulation studies carried out are intended to compare each of the measures against the set of criteria described in \autoref{meta analysis explainer}. The simulations are carried out according to the experiment descriptions previously explained, and the execution and results are presented here.

The scores given by `intermediate' metrics used to calculate final metrics (recall and precision for both F1 Score and BCubed F-Score) are also plotted for completeness, though they will not be subject to the result discussion.

\subsection{Dependence on Sample Size}
\label{sample size dependence}

Two of the experiments carried out in this thesis take particular note of sample size. Both involve increasing the sample size of the data, appending or prepending a number of points both prior to and following the ground truth change point. In each situation, a simulated algorithm detects a change $n$ points after the ground truth, simulating detection that is relevant, though slightly late, to ensure that there that the baseline metric value at $t = 0$ is not equal to $1$.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment1}
    \caption{Tail-Append Results Plot}
    \label{fig:Experiment1}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment4}
    \caption{Head-Prepend Results Plot}
    \label{fig:Experiment2}
\end{figure}

\autoref{fig:Experiment1} and \autoref{fig:Experiment2} show the results of this simulation study. Figure 1 demonstrates the value of metrics as the `head' of the data is increased, while Figure 2 demonstrates the value of metrics as the `tail' of the data is increased.

In this situation, we can see that both studies show a variation in metric score as the length of the data stream increases. For all of the clustering measures there is a clear increase in metric value as points are added. At $t=0$, the clustering metrics appear to correctly penalise for a late detection, though this penalty decreases as $t$ increases.

The Adjusted Rand Index shows a far more extreme temporal penalty for the change point detection, which increases sharply at the beginning of the experiment, before maintaining a rate of increase comparable to the other clustering measures.

The binary classification measures of Precision, Recall and F1 maintain a constant value of $0$ throughout the experiment, classifying the change point detection as a `missed' detection and penalising accordingly.

For these experiments (1 \& 2) the null hypothesis $H_0$ is rejected, and the alternative hypothesis $H_1$ is accepted.

\subsection{Impact of Data Preceding and Following a Known Change Point}

The experiment run for this criteria is similar to those run in \autoref{sample size dependence}, but differs in one important manner: the size of the time series is maintained at a constant value throughout. As points are added to the `head' of the data set, points are removed from the `tail'. In practice, this serves to move both the true and computed change points through the time series.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment2}
    \caption{Fixed Length Append/Prepend Results Plot}
    \label{fig:Experiment3}
\end{figure}

\autoref{fig:Experiment3} shows the results of the experiment. As with the previous experiment, the binary classification measures maintain a constant value of $0$, with variation only being shown by the clustering measures. It is interesting to note that the change in all clustering measures aside from Adjusted Rand Index is minimal, with an almost constant value being held throughout the experiment, aside from when the change points are at the extreme ends of the time series.

The Adjusted Rand Index exhibits behaviour unique to itself, with a value approaching 0 as the change points approach either end of time series. From position 150 to 350, the Adjusted Rand Index holds an almost constant value.

From these results it can be inferred that it is only the Adjusted Rand Index that is effected in any meaningful way by the position of the change point in a fixed length data set.

For this experiment (3), the null hypothesis $H_0$ fails to be rejected for several measures. The only situation in which the alternative hypothesis $H_1$ holds is with the Adjusted Rand Index, which shows a considerable range in values when the change point is at the extreme ends of the time series.

\subsection{Temporal Penalty}

This is experiment makes use of a fixed length data set with a single, static, known change point. A computed change point is moved through the time series, beginning at the known change point, and ending at the end of the time series. \autoref{fig:Experiment4} shows a plot of the results of this experiment:

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment3}
    \caption{Temporal Penalty Results Plot}
    \label{fig:Experiment4}
\end{figure}

At the start of the experiment, when the ground truth and the computed change point are equal, all of the measures return a value of $1$ as expected. Once the computed change point is moved through the time series, the binary classification measures immediately return a value of $0$, showing that none of the change points in the series (a single change point, in this case) have been detected.

All of the clustering measure values decrease as the computed change point moves away from the ground truth change point, though exhibit some strange behaviour as the computed change point approaches the end of the time series. In all cases, the clustering measures show an increase in score as the computed point approaches the end of the time series, with the temporal penalty for a late detection reducing.

The Adjusted Rand Index once again exhibits behaviour unique to itself, dropping below $0$ at approximately $t=278$. This behaviour is expected from the Adjusted Rand Index, being that the adjusted nature of the metric (allowing for cluster classification occurring by chance) results in a metric capable of returning a value $<0$.

The results of this experiment (4) cause the null hypothesis $H_0$ to be rejected. All of the measures show some issue with the application of a temporal penalty for late detections, and indeed also show issues with crediting algorithms for early detections. In this case, the alternative hypothesis $H_1$ holds.

\subsection{Penalisation for False Positives}

This experiment utilised a fixed length time series with a single ground truth change point, and a single computed change point that equals the ground truth change point. Successive false positives are added to the time series, moving from left to right across the time series.

\autoref{fig:Experiment5} shows the results of the experiment, with the score provided by each metric plotted against the number of false positives present in the data stream.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment5}
    \caption{False Positives Results Plot}
    \label{fig:Experiment5}
\end{figure}

The results of this experiment show that almost all of the metrics behave in a similar fashion. The binary classification measure of Recall maintains a value of $1$, while all of the other measures show a precipitous drop in score as the initial false positives are added. Precision and F1 scores for binary classification drop sharply, while the measures for clustering show a more gentle decrease in score. The Rand Index and BCubed F-Score end the experiment at approximately the same value, while the Adjusted Rand Index, F1 Score and Binary Classification Precision metrics also terminate at approximately the same value.

The results of this experiment cause the null hypothesis $H_0$ to hold, as all metrics behave correctly in this situation.

\subsection{Penalisation for False Negatives}

For this criteria evaluation, the experiment utilised a fixed length time series with multiple ground truth change points. As the experiment progressed, computed change points were added, equal to each of the ground truth change points. \autoref{fig:Experiment6} shows the results of the experiment, with the metric scores plotted against the number of false negatives present in the data stream.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment6}
    \caption{False Negatives Results Plot}
    \label{fig:Experiment6}
\end{figure}

The vast majority of measures see a consistent decrease in returned score as the number of false negatives increases. Indeed, once all of the ground truth change points have been correctly detected, all of the metrics return a score of $1$. Only the Adjusted Rand Index returns a score of $0$ after $10$ false negatives are found in the data stream. As Expected, BCubed Recall remained at a value of $1$ for the duration of the experiment.

\subsection{Number of Change Points in Fixed Length Data Stream}

This experiment adds change points to a data stream by `lifting' slices of the data stream - adding a value to them to create two change points. This is carried out multiple times, each time adding two ground truth change points, and two computed change points 3 time units after the ground truth change points. \autoref{fig:Experiment7} shows the results of this experiment, plotting the score returned by the metric against the number of change points in the stream.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/Experiment8}
    \caption{Change Point Density (Fixed Length) Results Plot}
    \label{fig:Experiment7}
\end{figure}

The results of this experiment show a drop in score provided by the Adjusted Rand Index and BCubed F-Score. The BCubed F-Score shows a linear rate of decline as change points are added to the stream, while the Adjusted Rand Index shows an increase in downward velocity towards the end of the experiment. All of the other metrics maintained a constant value of either $1$ or $0$, as expected.

\subsection{Number of Change Points in Variable Length Data Stream}

This final experiment is similar to the previous one, in that it increases the number of change points in a data stream. However, the difference here is that in this case, data is appended to the data stream, also increasing the length of the stream as each successive change point is added. The results of this experiment are shown in \autoref{fig:Experiment8}:

\begin{figure}[H]
    \includegraphics[width=\textwidth]{figures/Experiment7}
    \caption{Change Point Density (Variable Length) Results Plot}
    \label{fig:Experiment8}
\end{figure}

The results of the experiment show some interesting behaviour in the Rand Index, Adjusted Rand Index and BCubed F-Score metrics. The Rand index and the Adjusted Rand Index increase in value as data points and change points are added to the time series, while the BCubed F-Score decreases in value. The Adjusted Rand Index begins at a slightly lower level than the Rand Index and BCubed F-Score metrics, while the Rand Index and BCubed F-Score metrics diverge almost immediately.

Aside from the large change in score value at the beginning of the experiment, the change in value for the Rand Index, Adjusted Rand Index and BCubed F-Score metrics is small.

\section{Functional Requirements}

\section{Real-World Data Analysis}

As described in \autoref{real world explainer}, this experiment executed change detection algorithms against sets of real world data taken from Twitter. The data was gathered and filtered to be relevant to certain brands (again, as listed in \autoref{real world explainer}) and then annotated with ground truth change points by social media analysts from the host organisation. The ground truth annotations can be found in \autoref{groundtruth}.

What follows is the results of that study. For each data set, the scores provided by the evaluation measures are tabulated and provided in \autoref{fullscores}. The highest score(s) for each measure are highlighted for easy comparison and readability. Further discussion of the results and the extraction of conclusions will can be found in \autoref{results} and \autoref{conclusions}.

The final plots, including detected change points by each algorithm can also be found in \autoref{changeplots}.

Analysis of the raw data used for \autoref{fig:PrettyPlot} shows that there is not always consensus on the best algorithm. Some metrics provide higher scores for algorithms that perform objectively worse than others, when compared through by-eye analysis of the change point plots. Algorithms using variance as the test statistic tend to perform better than those using mean. Binary classification scores are universally low for all data sets.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/PrettyPlot}
    \caption{Average scores across all real-world data scenarios}
    \label{fig:PrettyPlot}
\end{figure}



\autoref{fig:PrettyPlot} shows a plot of average score values across all real world data sets, from each algorithm. The error bars are drawn to show 1 standard-deviation of error.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results}
\label{results}

This chapter is intended to provide a factual explanation of the results of each experiment carried out as a part of this research. It does not provide any discussion or conclusions based upon these results. The discussion and conclusions gleaned from these experiments can be found in \autoref{conclusions}.

\section{Metric Behaviour}

\begin{table}[h]
\centering
\begin{tabular}{@{}rcccc@{}}
\toprule
\textbf{Behaviour} & \multicolumn{1}{l}{\textbf{F1 Score}} & \multicolumn{1}{l}{\textbf{Rand Index}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}Adjusted\\ Rand Index\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}BCubed\\ F-Score\end{tabular}}} \\ \midrule
Credits correct detections & \cmark & \cmark & \cmark & \cmark \\
Penalises false negatives & \cmark & \cmark & \cmark & \cmark \\
Penalises false positives & \xmark & \cmark & \cmark & \cmark \\
Temporal penalty for late detections & \xmark & \mmark & \mmark & \cmark \\
Unaffected by sample size & \cmark & \xmark & \xmark & \xmark \\
Unaffected by change point density & \cmark & \cmark & \mmark & \mmark \\
Unaffected by data before/after change point & \cmark & \mmark & \xmark & \mmark \\ \bottomrule
\end{tabular}
\caption{Metric Behaviour Summary}
\label{tab:metric}
\end{table}

\todo[inline]{Missing `data before and after' affect}

\autoref{tab:metric} shows a summary of how the metrics being tested in this study behaved when the experiments were run. Behaviours exhibited by a given metric are denoted by \cmark, behaviours not exhibited as \xmark, and behaviours that are exhibited with some caveats are denoted by \mmark.

\subsection{F1 Score}

The F1 score is a `traditional' binary classification score, widely used for the evaluation of change point detection methods. Based upon the results of the experiments carried out, it is clear that this measure has some limitations in this domain.

Firstly, F1 score does not include any method for penalising a method based on late detections. Detections of a change point some number of points after the true change point may still be considered successful detections, depending on the functional requirements of the system that the method is implemented in. F1 Score returns a score of 0 for anything other than exact detections. However, the measure does properly penalise for false negatives. In a data set with two known change points, if one of these change points is successfully detected and the other not, the metric will return an F1 score of 0.5, which could be considered correct.

Secondly, F1 score does not appear to correctly penalise for false positives. In situations where a change point is detected some number of points after the true change point (so, as above, possibly a successful detection, albeit late), the metric continues to return a value of 0, even when false positives occur.

The F1 measure is, however, unaffected by changes in sample sizes in the experiments, and also exhibits no variation when the change point density in a given data set is increased or decreased.

\subsection{Rand Index}

The Rand Index is a clustering measure, calculated in these experiments by segmenting the data set around detected change points and treating all data points in each of these segmentations as a single cluster.

The Rand Index improves upon the F1 score, in that it successfully penalises for late detections, with some caveats. The Rand Index computed for a data set with a single late detection better reflects the true `accuracy' of a method, but it behaves inconsistently when this detected change point is near to the beginning or end of a time series. In the experiment testing temporal penalty, the Rand Index successfully provides a decreasing value as the detected change point moves away from the true change point, but starts to increase as it approaches the end of the data set.

Taken intuitively, this means that if a method detects a change point late, the Rand Index only penalises up to a certain point. If the detection occurs towards the end of the data set, the Rand Index may return a score that is actually \emph{higher} than that is returned if a method detects a change point late, but closer to the true change point. This is a clear limitation of the measure, and means it cannot be considered to completely fulfil this behaviour.

It is also clear, based on the results of these experiments, that the Rand Index is affected by the size of the data set, with a late detection being penalised higher in a small data set, than in a large. However, this difference is reasonably small - and it is safe to assume that in most situations, change detection methods will be compared in a like-for-like fashion, including ensuring that the test data sets are all of the same length.

\subsection{Adjusted Rand Index}

The Adjusted Rand Index is an additional clustering measure, calculated in a similar fashion to the `standard' Rand Index, but takes into account the possibility that some items in a cluster may be accidentally, or randomly classified into that cluster.

The Adjusted Rand Index behaved in much the same fashion as the Rand Index in most situations, though exhibited different behaviour in more than one scenario. The Adjusted Rand Index is more affected by dataset size, which is especially apparent in the experiment where change point density is increased while also increasing the data set size.

This measure also reacted inconsistently when both the true change point and computed change point are moved through the time series - showing a large, steep drop in returned score at both the beginning and end of the data set.

Temporal Penalties were more effectively scored by the Adjusted Rand Index, however - as even though the score returned by the measure increased as the detected change point moved towards the end of the time series, the value returned by the Adjusted Rand Index did not become higher than 0 at any one point.

\subsection{BCubed F-Score}

As a clustering measure, it was expected that BCubed would behave in much the same way as the Rand and Adjusted Rand Indices. However, it does exhibit some minor differences.

The BCubed metric is less heavily affected by the parabolic curve behaviour exhibited by clustering metrics when plotting temporal penalty - though it is affected. The results returned by BCubed show less variation as the computed change point moves towards the end of the time series, but it shows the same curve with approximately the same velocity as the Rand Indices.

The BCubed score also reacts differently to the Rand Index when change point density is increased with time series length, showing an almost linear drop, converging with the Adjusted Rand index at the end of the experiment, unlike the Rand Index, which remains constant.

When compared with the behaviour in a fixed length data set, the BCubed measure behaves in the opposite fashion to the Adjusted Rand Index. The scores start diverged, but quickly converge to the same value and remain almost constant towards the end of the experiment. BCubed also behaves in the opposite fashion to the `standard' Rand Index, with the scores diverging from the beginning of the experiment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Analysis \& Conclusions}
\label{conclusions}

\section{Conclusions}

There are multiple ways in which the measures evaluated in this thesis are shown to be inappropriate for use for evaluating change point detection approaches.

The clustering measures evaluated (Rand, Adjusted Rand, BCubed) show that they are often affected by the size of the data set, or by the density of change points in the data set, when scoring change points detected with some error.

Clustering measures especially exhibited inconsistent behaviour when penalising for late (or early) detections. In the experiments carried out, the fact that the score \emph{increases} as the detected change point approaches the end of the time series shows that these measures may be unreliable for providing scores for approaches in which detections are extremely late.

Binary Classification metrics performed well in simulation studies, fulfilling all but two of the criteria tested. However, when applied to real world data and change points detected therein, they proved to be ineffective.

These results do call into question some aspects of the validity of the experiments, which will be discussed in \autoref{threats}.

When the algorithms were applied to real world data sets, they behaved inconsistently, to the point where no measure agrees on which approach is `best' for all of the data sets evaluated. When the change point plots are examined by-eye, it is clear that the algorithms underperformed when used in this context. There were cases where the manual ground-truth annotations weer correctly detected by the algorithms, but there were also many situations in which the algorithm output did not closely match the ground truth at all. Despite this, the metrics being used still scored the algorithms highly in many cases (assuming a score > $0.5$ to be successful).

\section{The Ideal Metric}
\label{ideal metric}

\section{Threats to Validity}
\label{threats}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Future Work}

The results of this thesis open up numerous avenues for future research in the field. The validity issues discussed in \autoref{threats} suggest that changes to the experimental method could result in more interesting results.
It would also be interesting to formulate the `ideal metric' discussed in \autoref{ideal metric} mathematically, and prove it's veracity through simulation studies and analysis of real world data, much like that which was carried out in this thesis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography

\appendix
\addcontentsline{toc}{chapter}{APPENDICES}

\chapter{Pseudocode for Simulation Studies}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of increasing `head' length of a dataset on metrics}
\Begin{
    \ForEach{iteration}{
        Add 1 point to head of data, increasing length\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}


\begin{algorithm}[h]
\caption{Experiment to plot the effect of increasing `tail' length of a dataset on metrics}
\Begin{
    \ForEach{iteration}{
        Add 1 point to tail of data, increasing length\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of increasing `tail' length of a dataset on metrics}
\Begin{
    \ForEach{iteration}{
        Move $\tau$ and $\tau'$ 1 point further in time series\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of temporal distance between $\tau$ and $\tau'$ on metrics}
\Begin{
    \ForEach{iteration}{
        Leave $\tau$ unchanged\\
        $\tau' \leftarrow \tau' + 1$\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of increasing the number of false positive detections on metrics}
\Begin{
    \ForEach{iteration}{
        Add 1 additional false positive, such that $\tau'_{n+1} = \tau'_{n} + 5$\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of decreasing the number of false negative detections on metrics}
\Begin{
    \ForEach{iteration}{
        Add one additional $\tau'$ such that $\tau'_n = \tau_n$\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of the number of change points on metrics, by appending}
\Begin{

    $new\_point \leftarrow$ time series segment with 1 changepoint

    \ForEach{iteration}{
        Append $new\_point$ to data, increasing length\\
        Add additional $\tau$ at $tau_{new\_point}$\\
        Add additional $\tau'$ at $tau'{new\_point}$\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\begin{algorithm}[h]
\caption{Experiment to plot the effect of the number of change points on metrics, by adding 1 to slices of the time series \& maintaining constant time series length}
\Begin{
    \ForEach{iteration}{
        Add 1 to next slice of time series\\
        Add $\tau$ and $\tau'$ such that $\tau' = \tau + 2$\\
        Calculate metrics for current $\tau$, $\tau'$ against time series\\
        Store results
    }
}
\end{algorithm}

\chapter{Ground Truth Annotations}
\label{groundtruth}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/GroundTruthAnnotations}
    \caption{Unadjusted Ground Truth Plots}
    \label{fig:truth1}
\end{figure}

\chapter{Real-World Change Point Detection Plots}
\label{changeplots}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/dirkresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:dirk}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/ziggoresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:ziggo}
\end{figure}


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/bolresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:bol}
\end{figure}


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/connexxionresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:connexxion}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/dapresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:dap}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/jumboresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:jumbo}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/kvkresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:kvk}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/rabobankresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:rabobank}
\end{figure}


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/tele2results}
    \caption{`Dirk' Change Point Detections}
    \label{fig:tele2}
\end{figure}


\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/uwvresults}
    \caption{`Dirk' Change Point Detections}
    \label{fig:uwv}
\end{figure}

\chapter{Full Real World Data Scorings}
\label{fullscores}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.916384181 & 0.831326108 & 0.977777778 & 0.808522727 & 0.885131649 \\
MeanVar SegNeigh & 0 & 0 & NA & 0.774576271 & 0.57039828 & 0.977777778 & 0.662121212 & 0.789569859 \\
MeanVar PELT & 0 & 0 & NA & 0.750282486 & 0.528564301 & 0.977777778 & 0.572537879 & 0.722194622 \\
Var BinSeg & 0 & 0 & NA & \textbf{0.942937853} & \textbf{0.883671294} & \textbf{0.983333333} & \textbf{0.904356061} & \textbf{0.942192569} \\
Var SegNeigh & 0 & 0 & NA & \textbf{0.942937853} & \textbf{0.883671294} & \textbf{0.983333333} & \textbf{0.904356061} & \textbf{0.942192569} \\
Var PELT & 0 & 0 & NA & \textbf{0.942937853} & \textbf{0.883671294} & \textbf{0.983333333} & \textbf{0.904356061} & \textbf{0.942192569} \\
Mean BinSeg & 0 & 0 & NA & 0.73559322 & 0.503863154 & \textbf{0.983333333} & 0.564015152 & 0.716858425 \\
Mean SegNeigh & \textbf{0.2} & \textbf{1} & \textbf{0.333333333} & 0.93559322 & 0.8691606 & 1 & 0.875189394 & 0.933441067 \\
Mean PELT & 0.017241379 & \textbf{1} & 0.033898305 & 0.398305085 & 0.000746372 & 1 & 0.034090909 & 0.065934066 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Dirk' data set}
\label{tab:dirk}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0.2 & 0.2 & 0.2 & \textbf{0.819774011} & 0.54185782 & 0.656344086 & 0.855897436 & 0.742954366 \\
MeanVar SegNeigh & 0.2 & 0.2 & 0.2 & 0.785875706 & 0.477814329 & 0.619393939 & 0.842564103 & 0.713945385 \\
MeanVar PELT & 0.166666667 & 0.2 & 0.181818182 & 0.811864407 & \textbf{0.515766487} & 0.656344086 & 0.809230769 & 0.724812967 \\
Var BinSeg & 0.333333333 & 0.2 & 0.25 & 0.733333333 & 0.418172701 & 0.457575758 & \textbf{0.919230769} & 0.611004825 \\
Var SegNeigh & 0 & 0 & NA & 0.782485876 & 0.477512609 & 0.588624709 & 0.862564103 & 0.699738779 \\
Var PELT & 0 & 0 & NA & 0.782485876 & 0.477512609 & 0.588624709 & 0.862564103 & 0.699738779 \\
Mean BinSeg & \textbf{0.5} & \textbf{0.6} & \textbf{0.545454545} & 0.798305085 & 0.510612723 & \textbf{0.672727273} & 0.889230769 & \textbf{0.765974212} \\
Mean SegNeigh & 0.4 & 0.4 & 0.4 & 0.784745763 & 0.492259321 & 0.633660131 & 0.910064103 & 0.747117038 \\
Mean PELT & 0.083333333 & 1 & 0.153846154 & 0.807344633 & 0 & 1 & 0.1 & 0.181818182 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Ziggo' data set}
\label{tab:ziggo}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.363828662 & -0.033083215 & 0.380760369 & 0.838017984 & 0.523612904 \\
MeanVar SegNeigh & 0 & 0 & NA & 0.363828662 & -0.033083215 & 0.380760369 & 0.838017984 & 0.523612904 \\
MeanVar PELT & 0 & 0 & NA & 0.363828662 & -0.033083215 & 0.380760369 & 0.838017984 & 0.523612904 \\
Var BinSeg & 0 & 0 & NA & 0.332099418 & 0.002293403 & 0.341618191 & \textbf{0.969439728} & 0.505207652 \\
Var SegNeigh & 0 & 0 & NA & 0.332099418 & 0.002293403 & 0.341618191 & \textbf{0.969439728} & 0.505207652 \\
Var PELT & 0 & 0 & NA & 0.332099418 & 0.002293403 & 0.341618191 & \textbf{0.969439728} & 0.505207652 \\
Mean BinSeg & 0 & 0 & NA & 0.584346906 & 0.100269497 & 0.594594595 & 0.584795322 & 0.589654248 \\
Mean SegNeigh & \textbf{0.4} & 0.666666667 & \textbf{0.5} & \textbf{0.851930196} & \textbf{0.685228297} & \textbf{0.756503642} & 0.916871031 & \textbf{0.829002955} \\
Mean PELT & 0.048387097 & \textbf{1} & 0.092307692 & 0.6811211 & 0 & 1 & 0.064516129 & 0.121212121 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Bol.com' data set}
\label{tab:bol}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0.25 & 0.5 & 0.333333333 & 0.924907456 & 0.844343916 & 0.890083632 & 0.888137357 & 0.889109429 \\
MeanVar SegNeigh & 0.25 & 0.5 & 0.333333333 & 0.924907456 & 0.844343916 & 0.890083632 & 0.888137357 & 0.889109429 \\
MeanVar PELT & 0.142857143 & 0.5 & 0.222222222 & 0.812268641 & 0.590695386 & 0.890083632 & 0.666493236 & 0.762229907 \\
Var BinSeg & 0.333333333 & 0.5 & 0.4 & 0.904812269 & 0.805123425 & 0.846496107 & \textbf{0.920395421} & 0.881900364 \\
Var SegNeigh & 0.333333333 & 0.5 & 0.4 & 0.904812269 & 0.805123425 & 0.846496107 & \textbf{0.920395421} & 0.881900364 \\
Var PELT & 0.333333333 & 0.5 & 0.4 & 0.904812269 & 0.805123425 & 0.846496107 & \textbf{0.920395421} & 0.881900364 \\
Mean BinSeg & 0.166666667 & 0.5 & 0.25 & 0.87678477 & 0.727378477 & 0.983870968 & 0.677560738 & 0.802479376 \\
Mean SegNeigh & \textbf{0.4} & \textbf{1} & \textbf{0.571428571} & \textbf{0.958223162} & \textbf{0.910909287} & \textbf{1} & 0.881650681 & \textbf{0.937103459} \\
Mean PELT & 0.033898305 & \textbf{1} & 0.06557377 & 0.607086198 & 0.004865842 & \textbf{1} & 0.05450052 & 0.10336746 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Connexxion' data set}
\label{tab:connexxion}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.7106955 & 0.351202345 & 0.667180277 & 0.682527985 & 0.674766871 \\
MeanVar SegNeigh & 0 & 0 & NA & 0.698421975 & 0.334534305 & 0.655034895 & 0.703714426 & 0.678502647 \\
MeanVar PELT & \textbf{0.1} & 0.333333333 & \textbf{0.153846154} & 0.779661017 & 0.378254169 & 0.86440678 & 0.476111521 & 0.614022242 \\
Var BinSeg & 0 & 0 & NA & 0.790765634 & 0.562324673 & 0.667180277 & 0.884443977 & 0.760601126 \\
Var SegNeigh & 0 & 0 & NA & 0.77849211 & 0.543596211 & 0.655034895 & \textbf{0.905630417} & 0.760213636 \\
Var PELT & 0 & 0 & NA & 0.77849211 & 0.543596211 & 0.655034895 & \textbf{0.905630417} & 0.760213636 \\
Mean BinSeg & 0 & 0 & NA & \textbf{0.928696669} & \textbf{0.819906546} & 0.927966102 & 0.770037197 & \textbf{0.841657276} \\
Mean SegNeigh & 0 & 0 & NA & \textbf{0.920514319} & 0.805466429 & 0.850934376 & 0.818463347 & 0.834383069 \\
Mean PELT & 0.051724138 & \textbf{1} & 0.098360656 & 0.701344243 & 0.002735198 & \textbf{1} & 0.06927045 & 0.129565817 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Dakota Access Pipeline' data set}
\label{tab:dap}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.776837652 & 0.464020988 & 0.881048387 & 0.548984468 & 0.676462612 \\
MeanVar SegNeigh & 0 & 0 & NA & 0.837123215 & 0.627783501 & 0.859582543 & 0.703106332 & 0.773510247 \\
MeanVar PELT & 0 & 0 & NA & 0.837123215 & 0.627783501 & 0.859582543 & 0.703106332 & 0.773510247 \\
Var BinSeg & 0.333333333 & 0.5 & \textbf{0.4} & 0.935483871 & 0.85755302 & 0.923963134 & \textbf{0.884259259} & \textbf{0.903675299} \\
Var SegNeigh & 0.333333333 & 0.5 & \textbf{0.4} & 0.935483871 & 0.85755302 & 0.923963134 & \textbf{0.884259259} & \textbf{0.903675299} \\
Var PELT & 0.333333333 & 0.5 & \textbf{0.4} & 0.935483871 & 0.85755302 & 0.923963134 & \textbf{0.884259259} & \textbf{0.903675299} \\
Mean BinSeg & 0 & 0 & NA & 0.905341089 & 0.782728087 & 0.962365591 & 0.744311394 & 0.839408606 \\
Mean SegNeigh & 0.2 & 0.5 & 0.285714286 & \textbf{0.941300899} & \textbf{0.868296668} & \textbf{0.975806452} & 0.828540784 & 0.896163916 \\
Mean PELT & 0.032258065 & \textbf{1} & 0.0625 & 0.639344262 & 0 & 1 & 0.048387097 & 0.092307692 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Jumbo' data set}
\label{tab:jumbo}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rllllllll@{}}
\toprule
\textbf{Algorithm} & \multicolumn{1}{c}{\textbf{Precision}} & \multicolumn{1}{c}{\textbf{Recall}} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Rand}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}}} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.752542373 & 0.525490081 & 0.96969697 & 0.679861111 & 0.799316213 \\
MeanVar SegNeigh & 0 & 0 & NA & 0.551412429 & 0.241900508 & 0.96969697 & 0.432638889 & 0.598328306 \\
MeanVar PELT & 0 & 0 & NA & 0.528813559 & 0.214187614 & 0.96969697 & 0.404861111 & 0.571227361 \\
Var BinSeg & 0 & 0 & NA & 0.710169492 & 0.458374641 & 0.969047619 & 0.629861111 & 0.763477488 \\
Var SegNeigh & 0 & 0 & NA & \textbf{0.915254237} & \textbf{0.818859861} & 0.977777778 & \textbf{0.874305556} & \textbf{0.923151273} \\
Var PELT & 0 & 0 & NA & \textbf{0.915254237} & \textbf{0.818859861} & 0.977777778 & \textbf{0.874305556} & \textbf{0.923151273} \\
Mean BinSeg & 0 & 0 & NA & 0.736158192 & 0.502540289 & 0.977777778 & 0.654166667 & 0.783886525 \\
Mean SegNeigh & \textbf{0.2} & \textbf{1} & 0.333333333 & 0.746327684 & 0.519210875 & \textbf{1} & 0.665277778 & 0.798999166 \\
Mean PELT & 0.01754386 & \textbf{1} & \textbf{0.034482759} & 0.327118644 & 0.001636731 & \textbf{1} & 0.035416667 & 0.068410463 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Kamer van Koophandel' data set}
\label{tab:kvk}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & \textbf{0.333333333} & 0.333333333 & \textbf{0.333333333} & \textbf{0.849795441} & \textbf{0.667534638} & 0.740705303 & \textbf{0.935439137} & \textbf{0.826760168} \\
MeanVar SegNeigh & 0.2 & 0.333333333 & 0.25 & 0.845119813 & 0.653321966 & 0.753417168 & 0.867642527 & 0.806505494 \\
MeanVar PELT & 0.125 & 0.333333333 & 0.181818182 & 0.821157218 & 0.536777118 & \textbf{0.805488297} & 0.650385208 & 0.719674713 \\
Var BinSeg & \textbf{0.333333333} & 0.333333333 & \textbf{0.333333333} & \textbf{0.849795441} & \textbf{0.667534638} & 0.740705303 & \textbf{0.935439137} & \textbf{0.826760168} \\
Var SegNeigh & \textbf{0.333333333} & 0.333333333 & \textbf{0.333333333} & \textbf{0.849795441} & \textbf{0.667534638} & 0.740705303 & \textbf{0.935439137} & \textbf{0.826760168} \\
Var PELT & \textbf{0.333333333} & 0.333333333 & \textbf{0.333333333} & \textbf{0.849795441} & \textbf{0.667534638} & 0.740705303 & \textbf{0.935439137} & \textbf{0.826760168} \\
Mean BinSeg & 0.166666667 & 0.333333333 & 0.222222222 & 0.834599649 & 0.625980308 & 0.753417168 & 0.837134052 & 0.793072438 \\
Mean SegNeigh & 0.2 & 0.333333333 & 0.25 & 0.845119813 & 0.653321966 & 0.753417168 & 0.867642527 & 0.806505494 \\
Mean PELT & 0.051724138 & \textbf{1} & 0.098360656 & 0.715955582 & 0.002935273 & 1 & 0.069337442 & 0.129682997 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Rabobank' data set}
\label{tab:rabobank}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.799048123 & 0.561286116 & 0.803870968 & 0.753312212 & 0.777770817 \\
MeanVar SegNeigh & 0 & 0 & NA & 0.86938128 & 0.711729799 & 0.891935484 & \textbf{0.801699309} & 0.844413523 \\
MeanVar PELT & 0 & 0 & NA & 0.86938128 & 0.711729799 & 0.891935484 & \textbf{0.801699309} & 0.844413523 \\
Var BinSeg & 0 & 0 & NA & 0.740349022 & 0.459125414 & 0.698231009 & 0.745535714 & 0.721108398 \\
Var SegNeigh & 0 & 0 & NA & 0.740349022 & 0.459125414 & 0.698231009 & 0.745535714 & 0.721108398 \\
Var PELT & 0 & 0 & NA & 0.740349022 & 0.459125414 & 0.698231009 & 0.745535714 & 0.721108398 \\
Mean BinSeg & 0 & 0 & NA & 0.868852459 & 0.699871621 & 0.943548387 & 0.678715438 & 0.789515055 \\
Mean SegNeigh & 0 & 0 & NA & \textbf{0.925965098} & \textbf{0.835786141} & \textbf{0.954301075} & 0.785570276 & \textbf{0.861754013} \\
Mean PELT & \textbf{0.032258065} & \textbf{1} & \textbf{0.0625} & 0.626123744 & 0 & 1 & 0.048387097 & 0.092307692 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `Tele2' data set}
\label{tab:tele2}
\end{table}

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}rcccccccc@{}}
\toprule
\textbf{Algorithm} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Rand} & \textbf{\begin{tabular}[c]{@{}c@{}}Adjusted\\ Rand\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Precision\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ Recall\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}BCubed\\ FScore\end{tabular}} \\ \midrule
MeanVar BinSeg & 0 & 0 & NA & 0.779481756 & 0.488322974 & 0.709677419 & 0.779793907 & 0.743085305 \\
MeanVar SegNeigh & 0.2 & 0.333333333 & 0.25 & \textbf{0.947117927} & \textbf{0.869205369} & 0.925806452 & 0.84192055 & 0.881873135 \\
MeanVar PELT & 0.2 & 0.333333333 & 0.25 & \textbf{0.947117927} & \textbf{0.869205369} & 0.925806452 & 0.84192055 & 0.881873135 \\
Var BinSeg & 0.333333333 & 0.333333333 & 0.333333333 & 0.923849815 & 0.822681578 & 0.833870968 & 0.938694743 & 0.883183387 \\
Var SegNeigh & 0.333333333 & 0.333333333 & 0.333333333 & 0.923849815 & 0.822681578 & 0.833870968 & 0.938694743 & 0.883183387 \\
Var PELT & 0.333333333 & 0.333333333 & 0.333333333 & 0.923849815 & 0.822681578 & 0.833870968 & 0.938694743 & 0.883183387 \\
Mean BinSeg & 0.166666667 & 0.333333333 & 0.222222222 & 0.939185616 & 0.845868213 & 0.955645161 & 0.798611111 & \textbf{0.870099604} \\
Mean SegNeigh & \textbf{0.4} & 0.666666667 & \textbf{0.5} & 0.931782126 & 0.838129718 & 0.862007168 & \textbf{0.909124851} & 0.88493927 \\
Mean PELT & 0.05 & \textbf{1} & 0.095238095 & 0.705975674 & 0.005045764 & \textbf{1} & 0.068399044 & 0.128040257 \\ \bottomrule
\end{tabular}%
}
\caption{Scores for `UWV' data set}
\label{tab:uwv}
\end{table}

\end{document}
