%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{uvamscse}	% UvA thesis class

% numeric citations, name/title/year sorting
%\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, sorting=nty]{biblatex}

\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usetikzlibrary{positioning}

% program listings environment, see uvamscse.cls
\input{program-listings}
\newcommand{\cmd}[1]{\texttt{$\backslash$#1}}

\title{Detecting Online Conversations Going Viral}
\coverpic[250pt]{figures/Buzzcapture_thunder.png}
\subtitle{Time-series aware evaluation of change detection algorithms}
\date{Spring 2017}

\author{Matt Chapman}
\authemail{matthew.chapman@student.uva.nl}
\host{Buzzcapture International, \url{http://www.buzzcapture.com}}
\supervisor{Evangelos Kanoulas, Universiteit van Amsterdam}

\abstract{
	\todo[inline]{Write abstract}
}

\begin{document}
\nocite{*}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Problem Statement \& Motivation}

\section{Problem Statement}
\label{Problem Statement}

%Within the domain of change detection problems there are a number of available methods for evaluating the efficiency and accuracy of a given algorithm, depending on how the problem is framed. The different problem framings available are, for example:\unsure{are these definitions correct?}
%
%\begin{description}
%	\item[Classification] Wherein the algorithm result fits into one of two (or more) classes, for example correct or incorrect.
%	\item[Clustering] Wherein the algorithm results are formed into clusters and evaluated using clustering metrics.
%	\item[Partitioning] Wherein the data set is segmented according to change point location, and then evaluated in a similar way to clustering.
%	\item[Retrieval] Wherein the results of the algorithm are scored as a \emph{retrieval problem}, where detection relevance is taken into account, perhaps along with some form of temporal or redundancy penalty.
%\end{description}
%
%With this research it is intended to investigate this dichotomy between approaches and examine how they perform with relation to evaluating the performance of change detection algorithms, specifically when applied to time-series data from social media.
%
%While change detection itself has been around since the 1930's (and \emph{online} change detection since the 1950's) it is still a field that attracts new thoughts and approaches - one example of which being Ginsberg et. al.'s work on Influenza outbreak detection \cite{Ginsberg2009}.
%
%Various attempts have been made to evaluate the myriad approaches to change detection (\cite{Buntain2014} for example) but each of these attempts tend to frame the problem somewhat differently.
%
%This research intends to address the following research questions:
%
%\todo[inline]{These will change later in the project}

When asserting the veracity of an approach to a particular problem, this is almost always achieved by utilising some sort of metric or measure. To give an example, when evaluating the quality of some new piece of software, this could be done through calculating measures such as McCabe Complexity\todo{cite} or unit test coverage.

The problem of detecting change points in some data stream is no different. There exists within this field a number of metrics that may be used to prove the accuracy and effectiveness of a given approach, and these often fall into one of three categories: binary classification (e.g. F1 score), clustering (e.g. BCubed, or the Rand Index), or information retrieval (e.g. TREC retrieval score)\todo{cite this}.

The purpose of this thesis is twofold: firstly, to conduct a meta analysis of these measures, and how they perform in the domain of change point detection problems. Being that these measures are metrics designed for other problems that are not necessarily change point detection, it stands to reason that there are perhaps situations where families of metrics will disagree with each-other, or disagree with by-eye evaluation by domain experts. The second purpose is to evaluate the veracity of a number of existing change point detection algorithms when they are applied to data from social media.

This thesis will analyse three different change detection algorithms: \emph{Pruned Exact Linear Time}\todo{cite}, \emph{Binary Segmentation}\todo{cite} and \emph{Segment Neighbourhoods}\todo{cite}. These will be referred to as \emph{PELT}, \emph{BinSeg} and \emph{SegNeigh} respectively. The algorithms will be briefly discussed in this thesis, along with the chosen critical values such as \emph{minimum segment length}, \emph{penalty scoring} and \emph{assumed underlying distribution}.

The algorithms will then be applied to a collection of datasets falling into two categories: real-world conversation volume data taken from Twitter, and simulated data generated according to certain constraints which will also be discussed in \autoref{Research Method}.

The results provided by each technique will then be evaluated according to the following measures: Precision, Recall, F1 score, Rand Index, Adjusted Rand Index, Bcubed Precision, Bcubed Recall and Bcubed F-Score. Once these measures have been calculated, a meta-analysis will take place to evaluate the effectiveness of these methods when compared with each other and by-eye analysis from social media domain experts.

This thesis intends to answer the following research questions:

\begin{description}
	\item[RQ1] Are existing change point detection algorithms effective at detecting changes in social media data in a timely fashion?
	\item[RQ2] Are measures not specifically defined for the purpose of change detection evaluation effective in this domain?
	\item[RQ3] Do the aforementioned measures agree with by-eye analysis of results from change detection methods?
	\item[RQ4] If existing measures are deficient in some way, what would an `ideal' measure look like?\todo{reword}
\end{description}

\improvement[inline]{this needs expansion, I think}

\section{Motivation}

Change detection first came about as a quality control measure in manufacturing, and methods within this domain are generally referred to as \emph{control charts}. Since the inception of approaches such as CUSUM that provide the possibility for on-line evaluation of continuous data streams, change detection has grown as a field. With applications such as epidemic detection, online reputation management and infrastructure error detection, change detection is hugely useful both as an academic problem and in production systems of myriad application.

This particular research is motivated specifically by the online reputation management sector. The business hosting this research project (Buzzcapture International [\url{http://www.buzzcapture.com}]) is a Dutch consultancy that provides online reputation management services to other businesses throughout Europe. Chief among these services is the BrandMonitor application, which, among other features, provides a rudimentary notification system for clients that is triggered once there is an absolute increase in conversation volume (conversation volume being defined as the number of tweets relevant to the client over a given time period).

It is the intention of this thesis to not only answer the research questions set out in \autoref{Problem Statement}, but also to provide a robust recommendation and working prototype of a change detection methodology which could then eventually be implemented into the Brand Monitor tool to supply timely and relevant notifications to clients when a conversation concerning their brand exhibits a change in behaviour or otherwise ``goes viral''.

\chapter{Research Method}
\label{Research Method}

This thesis consists of three distinct parts:

\begin{itemize}
  \item Meta-analysis of evaluation measures
  \item Evaluation of change detection approaches based on the measures discussed in the previous part
  \item Application of change detection approaches to real world data and evaluation based on requirements elicitation and evaluation measures.
\end{itemize}


\section{Evaluation Measures}

As briefly discussed in the introduction to this thesis, there are a number of pre-existing approaches for the evaluation of change detection methods. Here, the measures being evaluated are briefly explained:

\begin{description}
	\item[F1 Score]\unsure{cite this?} This measure is utilised for testing accuracy in problems of binary classification. It considers two different measures, \emph{precision} and \emph{recall}, and takes a harmonic mean of the two measures to compute the final score.
	
	To calculate Precision and Recall, a \emph{confusion matrix} is first constructed to provide values for the total number of true positives, false negatives, false positives and true negatives:
	
	\begin{center}
	    \begin{tikzpicture}[box/.style={draw,rectangle,minimum size=2cm,text width=1.5cm,align=center}]
            \matrix (conmat) [row sep=.1cm,column sep=.1cm] {
            \node (tpos) [box,
               label=left:\( \mathbf{p'} \),
                label=above:\( \mathbf{p} \),
            ] {True \\ Positive};
            &
            \node (fneg) [box,
                label=above:\textbf{n},
                label=above right:\textbf{Totals},
                label=right:\( \mathrm{P}' \)] {False \\ Negative};
            \\
            \node (fpos) [box,
                label=left:\( \mathbf{n'} \),
                label=below left:\textbf{Totals},
                label=below:P] {False \\ Positive};
            &
            \node (tneg) [box,
                label=right:\( \mathrm{N}' \),
                label=below:N] {True \\ Negative};
            \\
            };
    
            \node [rotate=90,left=.05cm of conmat,anchor=center,text width=1.5cm,align=center] {
                \textbf{Actual Value}
            };
            \node [above=.05cm of conmat] {\textbf{Prediction Outcome}};
        \end{tikzpicture}
	\end{center}

	$Recall$ is computed as the number of correct positive results, divided by the number of positive results that should have been detected. $Precision$ is computed as the number of correct positive results divided by the number of all possible positive results:
	
	\begin{eqnarray}
	    Precision = \frac{TP}{TP + FP}\\
	    Recall = \frac{TP}{TP + FN}
	\end{eqnarray}
	
	The F1 score calculation (the harmonic mean of both recall and precision) can be described in general terms as follows:

	\begin{equation}
		F_1 = 2 \cdot \frac{1}{\frac{1}{recall} + \frac{1}{precision}} = 2 \cdot \frac{precision \cdot recall}{precision+recall}
		\label{equ:F1}
	\end{equation}

	As the F1 score is a binary classification measure, it can only be used to test the precision of an algorithm in a single domain, that is, was the change detected or not. Precision, Recall and F1 Score all provide a score $s$ such that $ \{s\in\mathbb{R} \mid 0\leq s \leq 1\} $

	\item[Rand Index] This measure is for computing the similarity between two clusters of data points. It is used for calculating the overall accuracy of a given clustering approach, when compared against a set of ground truth clusters. The Rand Index is defined as:

	\begin{equation}
		R = \frac{a+b}{a+b+c+d}
	\end{equation}
	
	Given a set of data points $S$, partitioned through two different methods, which we shall refer to as $X$ and $Y$. Based on this knowledge the following can be defined:
	
	\begin{itemize}
		\item $a$ = total number of pairs that were partitioned into the \emph{same} subset by both $X$ and $Y$
		\item $b$ = total number of pairs that were partitioned into \emph{different} subsets by both $X$ and $Y$
		\item $c$ = total number of pairs that were partitioned into the \emph{same} subset by $X$ and into a different subset by $Y$
		\item $d$ = total number of pairs that were partitioned into the \emph{same} subset by $Y$ and into a different subset by $X$
	\end{itemize}\todo{reword, wikipedia}
	
	Intuitively, it can be stated that $a+b$ is the total number of agreements between methods $X$ and $Y$, while $c+d$ is the total number of disagreements. This calculation will return $0$ for completely different clusters and $1$ for identical clusters. The Rand Index provides a score $s$ such that $ \{s\in\mathbb{R} \mid 0\leq s \leq 1\} $
	
	\item[Adjusted Rand Index]Similar to the Rand Index, but adjusted to take into account the random chance of pairs being assigned to the same cluster by both approaches being compared. While the Rand Index is suited for comparing a segmentation method against a known-good \emph{oracle} method, the adjusted index is more suited to comparing two differing approaches \cite{Matteson2012}. It is defined as:
	
	\begin{equation}
	    R_{adjusted} = \frac{R - R_{expected}}{R_{max} - R_{expected}}
	\end{equation}
	
	where $R_{expected}$ is defined as the expected Rand Index score for two completely random classifications of the given data\todo{cite, https://arxiv.org/pdf/1306.4933.pdf} and $R_{max}$ is defined as the maximum Rand Index value - generally $1$.
	
	The Adjusted Rand Index provides a score $s$ such that $ \{s\in\mathbb{R} \mid -1\leq s \leq 1\} $. This is different to the score returned by the `basic' Rand Index, in that it is possible for a negative value to be returned.
		
	\item[BCubed] BCubed is similar to the Adjusted Rand Index, in that it is adjusted to be appropriate for all constraints in a clustering problem. It was developed by Bagga and Baldwin\todo{cite} in 1998.
	
	BCubed operates in a similar way to the Recall and Precision classification metrics, in that it computes precision and recall values, before calculating a harmonic mean of the two metrics. BCubed also makes use of an additional \emph{correctness} function defined as follows, where $e$ and $e'$ are equivalent elements in a ground truth and computed clustering respectively, $L$ is a computed label and $C$ is a ground truth category:
	
	\begin{equation}
	\label{eqn:correctness}
	    \text{Correctness}(e,e') = 
	    \begin{cases}
	        1 & \text{iff $L(e) = L(e') \leftrightarrow C(e) = C(e')$}\\
	        0 & \text{otherwise}
	    \end{cases}
	\end{equation}
	
	\autoref{eqn:correctness} can be stated intuitively as returning $1$ if and only if both the ground truth and computed clusterings place the element $e$ into the same cluster, returning $0$ in all other cases.
	
	Precision and Recall can then be calculated as follows:
	
    \begin{align}
    \label{eqn:bcubed-precision}
        \text{Precision} &= Avg_e [Avg_{e' . C(e) = C(e')}[Correctness(e,e')]]\\
        \label{eqn:bcubed-recall}
        \text{Recall} &= Avg_e [Avg_{e' . L(e) = L(e')}[Correctness(e,e')]]
    \end{align}

To obtain the F score, a harmonic mean of Precision and Recall is taken in the same manner as the F1 score for binary classification:

\begin{equation}
    F_{bc} = 2 \cdot \frac{1}{\frac{1}{Recall_{bc}} + \frac{1}{Precision_{bc}}}
\end{equation}

\end{description}

\section{Evaluation Pipeline}

\subsection{Introduction}

The method of evaluating the approaches will be developed using a combination of Python and \textsf{R}. \textsf{R} is a combined language and environment created for the purpose of statistical computing \cite{RCoreTeam2017}. Thanks to the availability of the \texttt{changepoint} package in \textsf{R}\cite{Killick2014}, it is possible for for experiments with different change detection approaches to be carried out without the need to create a bespoke implementation.

\textsf{R} also allows for the generation of test data using method calls such as \texttt{rnorm()}. The \textsf{R} package \texttt{ggplot2} \cite{Wickham2009} also provides a number of powerful tools for data visualisation directly in \textsf{R}.

Python is being utilised also due to the availability of relevant software packages for the purposes of evaluation measure calculation.

\subsection{Changepoint Detection}

\texttt{changepoint} is a powerful R package that provides a number of different change detection algorithms, along with various approaches to penalty values. \texttt{changepoint} offers change detection in mean, variance and combinations of the two, using the AMOC, PELT, Binary Segmentation and Segment Neighbourhood algorithms\todo{cite these}.

\texttt{Changepoint} was developed by Rebecca Killick and Idris A. Eckley\todo{cite} and is provided free of charge under the GNU general public license \cite{Killick2014}.

\subsection{Calculation of Evaluation Measures}

The \texttt{ROCR} package in \textsf{R} is being utilised for the calculation of Precision, Recall and F1 scores when treating change detection as a classification problem. \texttt{ROCR} provides a number of functions, and is primarily developed for plotting two different performance measures against each other for the purposes of classification evaluation given a cut-off parameter. \texttt{ROCR} was developed by Tobias Sing et. al. and is provided free of charge under a GPL license\todo{cite}

For the calculation of the Rand Index and Adjusted Rand Index the \textsf{R} package \texttt{phyclust} is being used. This package was developed by Wei-Chen Chen, for the purposes of providing a phyloclustering implementation. While this approach is not something being examined in this thesis, the package does provide an implementation of the Rand Index and Adjusted Rand Index metrics, which are relevant to this research. This package is also provided free of charge under the GPL license.\todo{cite}

Calculating the BCubed precision, recall and f-score metrics is being carried out in Python, using the \texttt{python-bcubed} project. This is a small utility library for the calculation of BCubed metrics, developed by Hugo Hromic and provided under the MIT license.\todo{cite} In order to interface with this library via \textsf{R}, the package \texttt{rPython} is being used. This package provides functionality for running Python code and retrieving results in \textsf{R}, and was developed by Carlos J. Gil Bellosta. It is provided under a GPL-2 license.\todo{cite}

\section{Data Preparation}

Two distinct studies are being carried out in this thesis. The first is a simulation study carried out using simulated time series data with added change points at various time indices. This is generated in \textsf{R} and created to allow for a baseline comparison of change point detection methods and evaluation measures.

The simulated data is accompanied by a list of known change points. As the data is being generated programatically, it is not necessary to carry out annotation of this data in order to know at what time index these changes occur. This set of known change points is used as the \emph{ground truth} 

The second is being carried out using conversation volume data taken from Buzzcapture's Brand Monitor application. Brand Monitor harvests tweets via the Twitter API and makes them searchable through a bespoke interface as well as providing a number of useful metrics for reputation management. In order to gain data for this experiment, Social Media Analysts from Buzzcapture were requested to provide query strings for the Brand Monitor application, showing a situation where they feel a client should have been informed of a distinct change in conversation volume. The analysts were then asked to manually annotate the data with the points at which they believe a change point detection algorithm should detect a change. In this way, bias is avoided when establishing the ground truth for the data - the author was not involved in this annotation process other than providing instructions. The result of this exercise is then a set of time series' showing the twitter conversation volume over time for a given brand. This is accompanied by a separate data set containing indices at which changes \emph{should} be detected - thus serving as the ground truth for this part of the experiment.

The final result of the prepared data\todo{finish getting data together} shall be 2 different data sets of varying size and nature. For example, data sets with small/large changes in variance, and extremely large/small changes in mean.

\subsection{The Nature of Twitter Data}

%It is important, as part of this research, to understand the nature of the data being studied. The main reason for this is that several of the algorithms being evaluated require one to specify either a test statistic or the distribution of the data (as closely) as possible to give optimal results. For example, for detection of changes in mean using the \texttt{changepoint} package, one must specify whether the data follows a normal distribution, or whether to use the CUSUM (cumulative sum) test statistic that makes no assumptions about the distribution of the data.
%
%%\begin{figure}[hbt]
%%	\includegraphics[scale=0.5]{figures/ing-totals-full.png}
%%	\centering
%%	\caption{"ING" postings by date}
%%	\label{fig:ing_totals_graph}
%%\end{figure}
%
%Figure \ref{fig:ing_totals_graph} shows the distribution of twitter postings mentioning ``ING'' between 01/01/2017 and 28/02/2017. At first glance, this data has a number of interesting change points that we may wish to detect. Firstly, we shall examine what kind of distribution this data falls under. Carrying out the Shapiro-Wilk normality test gives us a \emph{p-value} of 0.0001037, well below the normally accepted 0.05. We can further see that this data set does not fit a normal distribution by generating a Q-Q plot of the data using R. \ref{fig:ing_QQ_plot} shows the completed QQ plot, which demonstrates the lack of normal distribution.
%
%%\begin{figure}[hbt]
%%	\includegraphics[scale=0.5]{figures/qqplot-ing.png}
%%	\centering
%%	\caption{Q-Q plot of ING postings}
%%	\label{fig:ing_QQ_plot}
%%\end{figure}
%
%The second type of data that will be used is entirely simulated. This will be achieved using R's facilities for generating sets of random data that fits a normal distribution. In this way it will be possible to demonstrate how (if at all) change detection algorithms handle data with different distribution models differently.

\section{Algorithm Configuration}

Change detection is an \emph{unbounded} problem. Left without some system of constraint, the algorithm could theoretically run to infinity. Indeed, one of the algorithms utilised in this research, PELT\todo{citation needed}, when left unbounded, will detect every data point in the time-series as a change point. This result is \textit{technically} correct, but not useful for our purposes. For this reason, the algorithms implement a penalty system, allowing for an optimal number of changepoints to be detected.

\subsection{Penalty Scores}

Penalty scores operate as a mechanism for optimising an unbounded problem such as the one being addressed here. Haynes et al. define the problem as follows \cite{Haynes2014}: Given time series data points $y_1,\ldots,y_n$, the series will contain $m$ changepoints such that their locations $\tau_{1:m} = (\tau_1,\ldots,\tau_m)$, where $\{\tau_i \in \mathbb{Z} \mid 1 \leqslant \tau_i \leqslant n-1\}$. $\tau_0$ is assumed to be 0, and $\tau_{m+1}$ is assumed to be $n$. In this way we can state that a given change detection algorithm will split the time series into $m + 1$ segments such that segment $i$ contains the points $y_{(\tau_{i-1}+1):\tau_i} = (y_{\tau_{i-1} + 1},\dots,y_{\tau_i})$.

\begin{equation}
\label{boundingfunc}
    Q_m(y_{1:n}) = \min_{\tau_{1:m}} \Bigg\{ \sum^{m+1}_{i=1}[C(y_{(\tau_{i - 1} + 1):\tau_i})] \Bigg\}
\end{equation}

\autoref{boundingfunc} is an equation showing the optimisation problem that change point detection presents. Intuitively, the equation shows that the optimal number of change points results from minimising the sum of the cost function.

There are a number of established approaches to calculating penalty values for unbounded problems, chiefly among which are Schwarz Information Criterion (SIC)/Bayesian Information Criterion (BIC), Akaike Information Criterion (AIC) and Hannan-Quinn\todo{cite}. Of these approaches, it is necessary to experiment to find the scheme that produces the `correct' number of changepoints for a given dataset. The penalty schemes are defined as follows:

\begin{align}
    \text{SIC} &= \ln(n) k - 2 \ln(\hat{L})\\
    \text{AIC} &= 2k - 2 \ln(\hat{L})\\
    \text{HQC} &= -2L_{max} + 2k \ln(\ln(n))
\end{align}

Where $n$ is the total number of observations, $k$ is the number of \emph{free parameters}\footnote{variables that are unable to be predicted by a given model} and $\hat{L}$ is the maximum value of a likelihood function.\todo{reword, wikipedia}

\subsection{Distribution Assumptions}

\section{Scoring Metrics}

\subsection{F1 Score}

The first calculated metric is \emph{F1 Score}, which is described by the equation \ref{equ:F1}. We calculate F1 score in the following manner:

For a given structure of time series data $S$ such that the points in the series are $(x_{t:0},\ldots,x_{t:n})$, we create two lists of points: $P$ and $T$. $P$ represents the set of changepoints predicted by the algorithm being evaluated, and $T$ represents the \emph{ground truth}, the changepoints annotated by a domain expert\todo{further explanation}. Both lists contain only $0$ and $1$, $0$ being a point where no changepoint is predicted, and $1$ being a point where a changepoint is predicted. In this manner we frame the changepoint detection problem as a binary classification problem.

From these data structures $P$ and $T$ we can easily compute precision as $\frac{\sum (P \wedge T) }{\sum P}$ and recall as $\frac{\sum (P \wedge T)}{\sum T}$. These values can then be inserted into equation \ref{equ:F1} to obtain the F1 score.

\subsection{Retrieval score}

Example function:

\begin{equation}
	f(relevance, temporal\_penalty, redundancy\_penalty)
\end{equation}

\improvement[inline]{start designing scoring methods}

Possible relevance measure, where $t_0$ is the earliest a spike can be detected and $t_n$ is the time that the signal returns to normal. $f(x)$ describes the function of the curve:

\begin{equation}
	\int^{t_0}_{t_n} f(x) dx
\end{equation}
\info{planning to do something with the area under the curve for relevance}
\todo[inline]{start desigining relevance measure}

\chapter{Background \& Context}

%This research was primarily borne out of reading \citetitle{Buntain2014} by \citeauthor{Buntain2014}\cite{Buntain2014}

\chapter{Research}

\section{Experimental Setup}

\chapter{Results}

\chapter{Analysis \& Conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{alphaurl}
\bibliography{../Bib/library.bib}

\newpage

\listoftodos[ToDo Notes]

\end{document}
