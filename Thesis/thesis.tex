%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{uvamscse}	% UvA thesis class

% numeric citations, name/title/year sorting
%\usepackage[backend=biber, style=alphabetic, citestyle=alphabetic, sorting=nty]{biblatex}

% program listings environment, see uvamscse.cls
\input{program-listings}
\newcommand{\cmd}[1]{\texttt{$\backslash$#1}}

\title{Detecting Online Conversations Going Viral}
\coverpic[250pt]{figures/Buzzcapture_thunder.png}
\subtitle{Time-series aware evaluation of change detection algorithms}
\date{Spring 2017}

\author{Matt Chapman}
\authemail{matthew.chapman@student.uva.nl}
\host{Buzzcapture International, \url{http://www.buzzcapture.com}}
\supervisor{Evangelos Kanoulas, Universiteit van Amsterdam}

\abstract{
	\todo[inline]{Write abstract}
}

\begin{document}
\nocite{*}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Problem Statement \& Motivation}

\section{Problem Statement}
\label{Problem Statement}

%Within the domain of change detection problems there are a number of available methods for evaluating the efficiency and accuracy of a given algorithm, depending on how the problem is framed. The different problem framings available are, for example:\unsure{are these definitions correct?}
%
%\begin{description}
%	\item[Classification] Wherein the algorithm result fits into one of two (or more) classes, for example correct or incorrect.
%	\item[Clustering] Wherein the algorithm results are formed into clusters and evaluated using clustering metrics.
%	\item[Partitioning] Wherein the data set is segmented according to change point location, and then evaluated in a similar way to clustering.
%	\item[Retrieval] Wherein the results of the algorithm are scored as a \emph{retrieval problem}, where detection relevance is taken into account, perhaps along with some form of temporal or redundancy penalty.
%\end{description}
%
%With this research it is intended to investigate this dichotomy between approaches and examine how they perform with relation to evaluating the performance of change detection algorithms, specifically when applied to time-series data from social media.
%
%While change detection itself has been around since the 1930's (and \emph{online} change detection since the 1950's) it is still a field that attracts new thoughts and approaches - one example of which being Ginsberg et. al.'s work on Influenza outbreak detection \cite{Ginsberg2009}.
%
%Various attempts have been made to evaluate the myriad approaches to change detection (\cite{Buntain2014} for example) but each of these attempts tend to frame the problem somewhat differently.
%
%This research intends to address the following research questions:
%
%\todo[inline]{These will change later in the project}

When asserting the veracity of an approach to a particular problem, this is almost always achieved by utilising some sort of metric or measure. To give an example, when evaluating the quality of some new piece of software, this could be done through calculating measures such as McCabe Complexity\todo{cite} or unit test coverage.

The problem of detecting change points in some data stream is no different. There exists within this field a number of metrics that may be used to prove the accuracy and effectiveness of a given approach, and these often fall into one of three categories: binary classification (e.g. F1 score), clustering (e.g. BCubed, or the Rand Index), or information retrieval (e.g. TREC retrieval score)\todo{cite this}.

The purpose of this thesis is twofold: firstly, to conduct a meta analysis of these measures, and how they perform in the domain of change point detection problems. Being that these measures are metrics designed for other problems that are not necessarily change point detection, it stands to reason that there are perhaps situations where families of metrics will disagree with each-other, or disagree with by-eye evaluation by domain experts. The second purpose is to evaluate the veracity of a number of existing change point detection algorithms when they are applied to data from social media.

This thesis will analyse three different change detection algorithms: \emph{Pruned Exact Linear Time}\todo{cite}, \emph{Binary Segmentation}\todo{cite} and \emph{Segment Neighbourhoods}\todo{cite}. These will be referred to as \emph{PELT}, \emph{BinSeg} and \emph{SegNeigh} respectively. The algorithms will be briefly discussed in this thesis, along with the chosen critical values such as \emph{minimum segment length}, \emph{penalty scoring} and \emph{assumed underlying distribution}.

The algorithms will then be applied to a collection of datasets falling into two categories: real-world conversation volume data taken from Twitter, and simulated data generated according to certain constraints which will also be discussed in \autoref{Research Method}.

The results provided by each technique will then be evaluated according to the following measures: Precision, Recall, F1 score, Rand Index, Adjusted Rand Index, Bcubed Precision, Bcubed Recall, Bcubed F-Score, and an information retrieval score based upon those in the \emph{TREC 2016} guidelines\todo{cite}. Once these measures have been calculated, a meta-analysis will take place to evaluate the effectiveness of these methods when compared with each other and by-eye analysis from social media domain experts.

This thesis intends to answer the following research questions:

\begin{description}
	\item[RQ1] Are existing change point detection algorithms effective at detecting changes in social media data in a timely fashion?
	\item[RQ2] Are measures not specifically defined for the purpose of change detection evaluation effective in this domain?
	\item[RQ3] Do the aforementioned measures agree with by-eye analysis of results from change detection methods?
\end{description}

\improvement[inline]{this needs expansion, I think}

\section{Motivation}

Change detection first came about as a quality control measure in manufacturing, and methods within this domain are generally referred to as \emph{control charts}. Since the inception of approaches such as CUSUM that provide the possibility for on-line evaluation of continuous data streams, change detection has grown as a field. With applications such as epidemic detection, online reputation management and infrastructure error detection, change detection is hugely useful both as an academic problem and in production systems of myriad application.

This particular research is motivated specifically by the online reputation management sector. The business hosting this research project (Buzzcapture International [\url{http://www.buzzcapture.com}]) is a Dutch consultancy that provides online reputation management services to other businesses throughout Europe. Chief among these services is the BrandMonitor application, which, among other features, provides a rudimentary notification system for clients that is triggered once there is an absolute increase in conversation volume (conversation volume being defined as the number of tweets relevant to the client over a given time period).

It is the intention of this thesis to not only answer the research questions set out in \autoref{Problem Statement}, but also to provide a robust recommendation and working prototype of a change detection methodology which could then eventually be implemented into the Brand Monitor tool to supply timely and relevant notifications to clients when a conversation concerning their brand exhibits a change in behaviour or otherwise ``goes viral''.

\chapter{Research Method}
\label{Research Method}

\section{Evaluation Measures}

As briefly discussed in the introduction to this thesis, there are a number of pre-existing approaches for the evaluation of change detection methods. Here, the measures being evaluated are briefly explained:

\begin{description}
	\item[F1 Score]\unsure{cite this?} This measure is utilised for testing accuracy in problems of binary classification. It considers two different measures, \emph{precision} and \emph{recall}. The F1 score can be described in general terms as follows:

	\begin{equation}
		F_1 = 2 \cdot \frac{1}{\frac{1}{recall} + \frac{1}{precision}} = 2 \cdot \frac{precision \cdot recall}{precision+recall}
		\label{equ:F1}
	\end{equation}

	$recall$ is computed as the number of correct positive results, divided by the number of positive results that should have been detected. $precision$ is computed as the number of correct positive results divided by the number of all possible positive results.

	As the F1 score is a binary classification measure, it can only be used to test the precision of an algorithm in a single domain, that is, was the change detected or not.
	
	When calculating an F1 score, it is generally accepted that a classification is given a score between 1 and 0, such that real numbers between 1 and 0 can be used to show to what degree something was classified into one group or another. The F1 score also results in a real number between 1 and 0.

	\item[Rand Index] This measure is for computing the similarity between two clusters of data points. It is used for calculating the overall accuracy of a given clustering approach, when compared against a set of ground truth clusters. The Rand Index is defined as:

	\begin{equation}
		R = \frac{a+b}{a+b+c+d}
	\end{equation}
	
	Given a set of data points $S$, partitioned through two different methods, which we shall refer to as $X$ and $Y$. Based on this knowledge the following can be defined:
	
	\begin{itemize}
		\item $a$ = total number of pairs that were partitioned into the \emph{same} subset by both $X$ and $Y$
		\item $b$ = total number of pairs that were partitioned into \emph{different} subsets by both $X$ and $Y$
		\item $c$ = total number of pairs that were partitioned into the \emph{same} subset by $X$ and into a different subset by $Y$
		\item $d$ = total number of pairs that were partitioned into the \emph{same} subset by $Y$ and into a different subset by $X$
	\end{itemize}
	
	Intuitively, it can be stated that $a+b$ is the total number of agreements between methods $X$ and $Y$, while $c+d$ is the total number of disagreements. This calculation will return $0$ for completely different clusters and $1$ for identical clusters.
	
	\item[Adjusted Rand Index]Similar to the Rand Index, but adjusted to take into account the random chance of pairs being assigned to the same cluster by both approaches being compared. While the Rand Index is suited for comparing a segmentation method against a known-good \emph{oracle} method, the adjusted index is more suited to comparing two differing approaches \cite{Matteson2012}. It is defined as:
	
	\begin{equation}
	    R_{adjusted} = \frac{R - R_{expected}}{R_{max} - R_{expected}}
	\end{equation}
	
	where $R_{expected}$ is defined as the expected Rand Index score for two completely random classifications of the given data\todo{cite, https://arxiv.org/pdf/1306.4933.pdf} and $R_{max}$ is defined as the maximum Rand Index value - generally $1$.

\end{description}

\section{Evaluation Pipeline}

\subsection{Introduction}

The method of evaluating the approaches will be developed using a combination of Python and \textsf{R}. \textsf{R} is a combined language and environment created for the purpose of statistical computing \cite{RCoreTeam2017}. Thanks to the availability of the \texttt{changepoint} package in \textsf{R}\cite{Killick2014}, it is possible for for experiments with different change detection approaches to be carried out without the need to create a bespoke implementation.

\textsf{R} also allows for the generation of test data using method calls such as \texttt{rnorm()}. The \textsf{R} package \texttt{ggplot2} \cite{Wickham2009} also provides a number of powerful tools for data visualisation directly in \textsf{R}.

Python is being utilised also due to the availability of relevant software packages for the purposes of evaluation measure calculation.

\subsection{Changepoint Detection}

\texttt{changepoint} is a powerful R package that provides a number of different change detection algorithms, along with various approaches to penalty values. \texttt{changepoint} offers change detection in mean, variance and combinations of the two, using the AMOC, PELT, Binary Segmentation and Segment Neighbourhood algorithms\todo{cite these}.

\texttt{Changepoint} was developed by Rebecca Killick and Idris A. Eckley\todo{cite} and is provided free of charge under the GNU general public license \cite{Killick2014}.

\subsection{Calculation of Evaluation Measures}

The \texttt{ROCR} package in \textsf{R} is being utilised for the calculation of Precision, Recall and F1 scores when treating change detection as a classification problem. \texttt{ROCR} provides a number of functions, and is primarily developed for plotting two different performance measures against each other for the purposes of classification evaluation given a cut-off parameter. \texttt{ROCR} was developed by Tobias Sing et. al. and is provided free of charge under a GPL license\todo{cite}

For the calculation of the Rand Index and Adjusted Rand Index the \textsf{R} package \texttt{phyclust} is being used. This package was developed by Wei-Chen Chen, for the purposes of providing a phyloclustering implementation. While this approach is not something being examined in this thesis, the package does provide an implementation of the Rand Index and Adjusted Rand Index metrics, which are relevant to this research. This package is also provided free of charge under the GPL license.\todo{cite}

Calculating the BCubed precision, recall and f-score metrics is being carried out in Python, using the \texttt{python-bcubed} project. This is a small utility library for the calculation of BCubed metrics, developed by Hugo Hromic and provided under the MIT license.\todo{cite} In order to interface with this library via \textsf{R}, the package \texttt{rPython} is being used. This package provides functionality for running Python code and retrieving results in \textsf{R}, and was developed by Carlos J. Gil Bellosta. It is provided under a GPL-2 license.\todo{cite}

\section{Data Preparation}

There are two data sets that will be used in this research. The first is a collection of 30.9M Dutch language tweets collected between 01/01/2017 and 28/02/2017. The data will be filtered for certain terms in the tweet 'body' to generate collections of tweets that would be considered 'relevant' to a particular business or individual. In this way, I will be able to simulate BrandMonitor queries myself, without the need of the software itself.

%\begin{figure}[hbt]
%	\includegraphics[scale=0.5]{figures/twitter-totals-full.png}
%	\centering
%	\caption{Twitter Data Set}
%	\label{fig:totals_graph}
%\end{figure}

\improvement[inline]{Make graph prettier}

The final result of the prepared data\todo{finish getting data together} shall be 5 different subsets of varying size and nature. For example, data sets with large changes in variance (noise), and extremely large/small changes in mean.

\subsection{The Nature of Twitter Data}

It is important, as part of this research, to understand the nature of the data being studied. The main reason for this is that several of the algorithms being evaluated require one to specify either a test statistic or the distribution of the data (as closely) as possible to give optimal results. For example, for detection of changes in mean using the \texttt{changepoint} package, one must specify whether the data follows a normal distribution, or whether to use the CUSUM (cumulative sum) test statistic that makes no assumptions about the distribution of the data.

%\begin{figure}[hbt]
%	\includegraphics[scale=0.5]{figures/ing-totals-full.png}
%	\centering
%	\caption{"ING" postings by date}
%	\label{fig:ing_totals_graph}
%\end{figure}

Figure \ref{fig:ing_totals_graph} shows the distribution of twitter postings mentioning ``ING'' between 01/01/2017 and 28/02/2017. At first glance, this data has a number of interesting change points that we may wish to detect. Firstly, we shall examine what kind of distribution this data falls under. Carrying out the Shapiro-Wilk normality test gives us a \emph{p-value} of 0.0001037, well below the normally accepted 0.05. We can further see that this data set does not fit a normal distribution by generating a Q-Q plot of the data using R. \ref{fig:ing_QQ_plot} shows the completed QQ plot, which demonstrates the lack of normal distribution.

%\begin{figure}[hbt]
%	\includegraphics[scale=0.5]{figures/qqplot-ing.png}
%	\centering
%	\caption{Q-Q plot of ING postings}
%	\label{fig:ing_QQ_plot}
%\end{figure}

The second type of data that will be used is entirely simulated. This will be achieved using R's facilities for generating sets of random data that fits a normal distribution. In this way it will be possible to demonstrate how (if at all) change detection algorithms handle data with different distribution models differently.

\section{Algorithm Configuration}

Change detection is an \emph{unbounded} problem. Left without some system of constraint, the algorithm could theoretically run to infinity. Indeed, one of the algorithms utilised in this research, PELT\todo{citation needed}, when left unbounded, will detect every data point in the time-series as a change point. This result is \textit{technically} correct, but not useful for our purposes. For this reason, the algorithms implement a penalty system, allowing for an optimal number of changepoints to be detected.

\subsection{Penalty Scores}

Penalty scores operate as a mechanism for optimising an unbounded problem such as the one being addressed here. Haynes et al. define the problem as follows \cite{Haynes2014}: Given time series data points $y_1,\ldots,y_n$, the result of a given algorithm shall be a set of $m$ changepoints such that their locations $\tau_{1:m} = (\tau_1,\ldots,\tau_m)$, where $\tau_i$ is an integer between 1 and $n-1$ inclusive.

\begin{equation}
    Q_m(y_{1:n}) = \min_{\tau_{1:m}} \Bigg\{ \sum^{m+1}_{i=1}[C(y_{(\tau_{i - 1} + 1):\tau_i})] \Bigg\}
\end{equation}

There are a number of established approaches to calculating penalty values for unbounded problems, chiefly among which are Schwarz Information Criterion (SIC), Bayesian Information Criterion (BIC), Akaike Information Criterion (AIC) and Hannan-Quinn\todo{cite}. Of these approaches, it is necessary to experiment to find the scheme that produces the `correct' number of changepoints for a given dataset.

\section{Scoring Metrics}

\subsection{F1 Score}

The first calculated metric is \emph{F1 Score}, which is described by the equation \ref{equ:F1}. We calculate F1 score in the following manner:

For a given structure of time series data $S$ such that the points in the series are $(x_{t:0},\ldots,x_{t:n})$, we create two lists of points: $P$ and $T$. $P$ represents the set of changepoints predicted by the algorithm being evaluated, and $T$ represents the \emph{ground truth}, the changepoints annotated by a domain expert\todo{further explanation}. Both lists contain only $0$ and $1$, $0$ being a point where no changepoint is predicted, and $1$ being a point where a changepoint is predicted. In this manner we frame the changepoint detection problem as a binary classification problem.

From these data structures $P$ and $T$ we can easily compute precision as $\frac{\sum (P \wedge T) }{\sum P}$ and recall as $\frac{\sum (P \wedge T)}{\sum T}$. These values can then be inserted into equation \ref{equ:F1} to obtain the F1 score.

\subsection{Retrieval score}

Example function:

\begin{equation}
	f(relevance, temporal\_penalty, redundancy\_penalty)
\end{equation}

\improvement[inline]{start designing scoring methods}

Possible relevance measure, where $t_0$ is the earliest a spike can be detected and $t_n$ is the time that the signal returns to normal. $f(x)$ describes the function of the curve:

\begin{equation}
	\int^{t_0}_{t_n} f(x) dx
\end{equation}
\info{planning to do something with the area under the curve for relevance}
\todo[inline]{start desigining relevance measure}

\chapter{Background \& Context}

%This research was primarily borne out of reading \citetitle{Buntain2014} by \citeauthor{Buntain2014}\cite{Buntain2014}

\chapter{Research}

\section{Experimental Setup}

\chapter{Results}

\chapter{Analysis \& Conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{alphaurl}
\bibliography{../Bib/library.bib}

\newpage

\listoftodos[ToDo Notes]

\end{document}
